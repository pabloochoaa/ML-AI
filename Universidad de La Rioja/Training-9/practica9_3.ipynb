{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IA2122/practica-9-paochoa/blob/main/practica9_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHXROQV9uZ2n"
      },
      "source": [
        "# Práctica 9 Parte 3: Desarrollando un modelo de lenguaje para generar texto\n",
        "\n",
        "Un modelo de lenguaje puede predecir la siguiente palabra de una secuencia basándose en palabras observadas anteriormente. Las redes neuronales son el método más utilizado para desarrollar este tipo de modelos porque pueden usar una representación donde palabras con significados similares tienen representaciones similares. \n",
        "\n",
        "En esta parte de la práctica vamos a ver cómo generar uno de esos modelos. \n",
        "\n",
        "Este notebook está basado en el libro Deep Learning for Natural Language Processing de Jason Brownlee. \n",
        "\n",
        "Es importante que tengas activado el uso de **GPU** en el notebook de colab (menú Edit -> Notebook Settings -> Hardware accelerator)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7fHn2VKwO0E"
      },
      "source": [
        "## La República de Platón\n",
        "\n",
        "Nuestro modelo de lenguaje va a estar basado en la república de Platón. Este libro está estructurado en forma de una conversación que trata el tema del orden y la justicia dentro de una ciudad. El texto completo está disponible para el dominio público dentro del [proyecto Gutenberg](http://www.gutenberg.org/).\n",
        "\n",
        "Este libro de Platón está disponible en varios formatos en el [proyecto Gutenberg](http://www.gutenberg.org/cache/epub/1497/pg1497.txt). La versión que nos interesa a nosotros es la versión ASCII del libro. Con la siguiente instrucción puedes descargar el libro donde se han eliminado la portada y la contraportada. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HpmhqeiuuFYV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c22c3d97-f64d-44e0-8427-9e74626d0ebb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-25 16:15:08--  https://raw.githubusercontent.com/ts1819/datasets/master/practica5/republic.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 657826 (642K) [text/plain]\n",
            "Saving to: ‘republic.txt’\n",
            "\n",
            "\rrepublic.txt          0%[                    ]       0  --.-KB/s               \rrepublic.txt        100%[===================>] 642.41K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2022-05-25 16:15:08 (21.1 MB/s) - ‘republic.txt’ saved [657826/657826]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/ts1819/datasets/master/practica5/republic.txt -O republic.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hb_hckdhxWdz"
      },
      "source": [
        "## Preparación de los datos\n",
        "\n",
        "Vamos a preparar los datos para construir nuestro modelo. \n",
        "\n",
        "### Revisando el texto\n",
        "\n",
        "Vamos a comenzar revisando parte del texto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kn3g3XJYxSTO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "343b71d6-7938-4194-ecf0-913172eeed29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BOOK I.\r\n",
            "\r\n",
            "I went down yesterday to the Piraeus with Glaucon the son of Ariston,\r\n",
            "that I might offer up my prayers to the goddess (Bendis, the Thracian\r\n",
            "Artemis.); and also because I wanted to see in what manner they would\r\n",
            "celebrate the festival, which was a new thing. I was delighted with the\r\n",
            "procession of the inhabitants; but that of the Thracians was equally,\r\n",
            "if not more, beautiful. When we had finished our prayers and viewed the\r\n",
            "spectacle, we turned in the direction of the city; and at that instant\r\n",
            "Polemarchus the son of Cephalus chanced to catch sight of us from a\r\n",
            "distance as we were starting on our way home, and told his servant to\r\n",
            "run and bid us wait for him. The servant took hold of me by the cloak\r\n",
            "behind, and said: Polemarchus desires you to wait.\r\n",
            "\r\n",
            "I turned round, and asked him where his master was.\r\n",
            "\r\n",
            "There he is, said the youth, coming after you, if you will only wait.\r\n",
            "\r\n",
            "Certainly we will, said Glaucon; and in a few minutes Polemarchus\r\n",
            "appeared, and with him Adeimantus, Glaucon's brother, Niceratus the son\r\n",
            "of Nicias, and several others who had been at the procession.\r\n",
            "\r\n",
            "Polemarchus said to me: I perceive, Socrates, that you and your\r\n",
            "companion are already on your way to the city.\r\n",
            "\r\n",
            "You are not far wrong, I said.\r\n",
            "\r\n",
            "But do you see, he rejoined, how many we are?\r\n",
            "\r\n",
            "Of course.\r\n"
          ]
        }
      ],
      "source": [
        "!head -30 republic.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4uFv8RWzVn-"
      },
      "source": [
        "A partir de un rápido vistazo al fragmento de texto anterior podemos ver ciertas cuestiones que tendremos que procesar:\n",
        "- Las cabeceras de los capítulos.\n",
        "- Muchos signos de puntuación.\n",
        "- Nombres extraños.\n",
        "- Algunos monólogos muy largos. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiAfXxOz0GQT"
      },
      "source": [
        "### Cargando el texto\n",
        "\n",
        "El primer paso consiste en cargar el texto en memoria. Podemos desarrollar una pequeña función que se encargue de esto. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_dDTRFi4xmBr"
      },
      "outputs": [],
      "source": [
        "def load_doc(filename):\n",
        "  # Abrimos el fichero en modo lectura\n",
        "  file = open(filename,'r')\n",
        "  # Leemos el texto completo\n",
        "  text = file.read()\n",
        "  # Cerramos el fichero\n",
        "  file.close()\n",
        "  return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SdKtPFM0aZ-"
      },
      "source": [
        "Usando dicha función podemos cargar nuestro fichero del siguiente modo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "x_oUjl_Q0Zgn"
      },
      "outputs": [],
      "source": [
        "in_filename = 'republic.txt'\n",
        "doc = load_doc(in_filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVfi8WHI0kKb"
      },
      "source": [
        "Ahora podemos mostrar parte de dicho texto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8ZRb4q6W0jQT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fcb1914-c0ec-463a-de06-63f6f178c43d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BOOK I.\n",
            "\n",
            "I went down yesterday to the Piraeus with Glaucon the son of Ariston,\n",
            "that I might offer up my prayers to the goddess (Bendis, the Thracian\n",
            "Artemis.); and also because I wanted to see in what\n"
          ]
        }
      ],
      "source": [
        "print(doc[:200])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wj4PZ32G0reo"
      },
      "source": [
        "### Limpiando el texto\n",
        "\n",
        "Ahora necesitamos transformar el texto en bruto a una secuencia de tokens (o palabras) que podamos usar para entrenar nuestro modelo. \n",
        "\n",
        "Vamos a aplicar las siguientes operaciones para limpiar nuestro texto:\n",
        "- Reemplazar todas las ocurrencias de '-' con un espacio en blanco de manera que podamos partir mejor las palabras.\n",
        "- Partir las palabras basándonos en espacios en blanco.\n",
        "- Eliminar todos los símbolos de puntuación.\n",
        "- Eliminar todas las palabras que no son alfabéticas. \n",
        "- Normalizar todas las palabras a minúsculas.\n",
        "\n",
        "La mayoría de estas transformaciones tienen como objetivo reducir el tamaño del vocabulario. Un tamaño de vocabulario excesivamente grande es un problema cuando se intenta crear modelos de lenguaje. Vocabularios pequeños producen modelos más pequeños que se entrenan más rápidos.\n",
        "\n",
        "Vamos a implementar todas las operaciones de limpieza en la siguiente función."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "NsZiyRsE0niT"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import re\n",
        "\n",
        "def clean_doc(doc):\n",
        "  # Reemplazar '--' con un espacio en blanco ' '\n",
        "  doc = doc.replace('--',' ')\n",
        "  # Partir palabras basándonos en espacios en blanco\n",
        "  tokens = doc.split()\n",
        "  # Vamos a escapar las palabras para poder filtrarlas por caracteres\n",
        "  re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "  # Eliminamos los símbolos de puntuación\n",
        "  tokens = [re_punc.sub('',w) for w in tokens]\n",
        "  # Eliminamos elementos que nos son alfabéticos\n",
        "  tokens = [word for word in tokens if word.isalpha()]\n",
        "  # Convertimos a minúsculas\n",
        "  tokens = [word.lower() for word in tokens]\n",
        "  return tokens\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVV2RsWB2kOD"
      },
      "source": [
        "Procedemos a limpiar nuestro documento y a continuación mostramos algunas estadísticas sobre nuestro vocabulario."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wUUCtGKP2jdA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "120e7c24-d381-41e9-85b1-449477055830"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['book', 'i', 'i', 'went', 'down', 'yesterday', 'to', 'the', 'piraeus', 'with', 'glaucon', 'the', 'son', 'of', 'ariston', 'that', 'i', 'might', 'offer', 'up', 'my', 'prayers', 'to', 'the', 'goddess', 'bendis', 'the', 'thracian', 'artemis', 'and', 'also', 'because', 'i', 'wanted', 'to', 'see', 'in', 'what', 'manner', 'they', 'would', 'celebrate', 'the', 'festival', 'which', 'was', 'a', 'new', 'thing', 'i', 'was', 'delighted', 'with', 'the', 'procession', 'of', 'the', 'inhabitants', 'but', 'that', 'of', 'the', 'thracians', 'was', 'equally', 'if', 'not', 'more', 'beautiful', 'when', 'we', 'had', 'finished', 'our', 'prayers', 'and', 'viewed', 'the', 'spectacle', 'we', 'turned', 'in', 'the', 'direction', 'of', 'the', 'city', 'and', 'at', 'that', 'instant', 'polemarchus', 'the', 'son', 'of', 'cephalus', 'chanced', 'to', 'catch', 'sight', 'of', 'us', 'from', 'a', 'distance', 'as', 'we', 'were', 'starting', 'on', 'our', 'way', 'home', 'and', 'told', 'his', 'servant', 'to', 'run', 'and', 'bid', 'us', 'wait', 'for', 'him', 'the', 'servant', 'took', 'hold', 'of', 'me', 'by', 'the', 'cloak', 'behind', 'and', 'said', 'polemarchus', 'desires', 'you', 'to', 'wait', 'i', 'turned', 'round', 'and', 'asked', 'him', 'where', 'his', 'master', 'was', 'there', 'he', 'is', 'said', 'the', 'youth', 'coming', 'after', 'you', 'if', 'you', 'will', 'only', 'wait', 'certainly', 'we', 'will', 'said', 'glaucon', 'and', 'in', 'a', 'few', 'minutes', 'polemarchus', 'appeared', 'and', 'with', 'him', 'adeimantus', 'glaucons', 'brother', 'niceratus', 'the', 'son', 'of', 'nicias', 'and', 'several', 'others', 'who', 'had', 'been', 'at', 'the', 'procession', 'polemarchus', 'said']\n"
          ]
        }
      ],
      "source": [
        "tokens = clean_doc(doc)\n",
        "print(tokens[:200])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "MOBzRk232uAv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b608c947-05ae-4339-d045-448e8223e4a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Tokens: 118684\n",
            "Unique Tokens: 7409\n"
          ]
        }
      ],
      "source": [
        "print('Total Tokens: %d' % len(tokens))\n",
        "print('Unique Tokens: %d' %len(set(tokens)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvkoWbVh25cR"
      },
      "source": [
        "Es decir, nuestro modelo consta de una 7500 palabras. Este tamaño de vocabulario es pequeño y va a ser manejable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odlhVwpz3CQi"
      },
      "source": [
        "### Guardando el texto limpio\n",
        "\n",
        "Vamos a organizar la larga lista de tokens en secuencias de 50 palabras de entrada y 1 palabra de salida (esto servirá para luego entrenar nuestro modelo). \n",
        "\n",
        "Este proceso lo implementamos con la siguiente función."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "qtEe2J7E21am"
      },
      "outputs": [],
      "source": [
        "def organize_tokens(tokens,input_len=50,output_len=1):\n",
        "  length = input_len + output_len\n",
        "  sequences = list()\n",
        "  for i in range(length,len(tokens)):\n",
        "    # Elegimos la secuencia de tokens\n",
        "    seq = tokens[i-length:i]\n",
        "    # Convertimos la secuencia en una línea\n",
        "    line = ' '.join(seq)\n",
        "    # Almacenamos el resultado\n",
        "    sequences.append(line)\n",
        "  return sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6upZchl37wM"
      },
      "source": [
        "Organizamos nuestros tokens. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jpcwuFBp35XJ"
      },
      "outputs": [],
      "source": [
        "lines = organize_tokens(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXyDfPjA4Bfr"
      },
      "source": [
        "Ahora vamos a guardar las secuencias en un nuevo fichero para poder cargarlo en el futuro. Para ello nos definimos la siguiente función que guardará cada elemento de la secuencia en una línea del fichero. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "SWxPoCvn4AfS"
      },
      "outputs": [],
      "source": [
        "def save_doc(lines,filename):\n",
        "  data = '\\n'.join(lines)\n",
        "  file = open(filename,'w')\n",
        "  file.write(data)\n",
        "  file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtdjRviE4Wfd"
      },
      "source": [
        "Podemos llamar a la función anterior para guardar nuestro fichero."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "rbncNaRo4T74"
      },
      "outputs": [],
      "source": [
        "out_filename = 'republic_sequences.txt'\n",
        "save_doc(lines,out_filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giGMZ3yW4m9O"
      },
      "source": [
        "Podemos ver parte de dicho fichero."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "hitWeMb84e1R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05f993d1-061e-46a1-ad43-64b1a6f53e06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "book i i went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was\n",
            "i i went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was delighted\n",
            "i went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was delighted with\n",
            "went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was delighted with the\n",
            "down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was delighted with the procession\n"
          ]
        }
      ],
      "source": [
        "!head -5 republic_sequences.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-Do4ht04u3Q"
      },
      "source": [
        "## Entrenando el modelo de lenguaje\n",
        "\n",
        "Vamosa  entrenar ahora nuestro modelo a partir de los datos que hemos preparado. Dicho modelo tendrá ciertas características:\n",
        "- Usará una representación para las palabras de manera que palabras diferentes con significados similares tendrán una representación similar.\n",
        "- La representación será aprendida al mismo tiempo que se aprende el modelo.\n",
        "- Aprenderá a predecir la probabilidad de la siguiente palabra a partir del contexto de las últimas 100 palabras.\n",
        "\n",
        "En concreto para implementar este modelo vamos a usar una capa de Embedding para aprender la representación de las palabras, y una red neuronal recurrente con capas LSTM para predecir nuevas palabras basándonos en el contexto. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ss_vMHL65cx2"
      },
      "source": [
        "### Cargando las secuencias\n",
        "\n",
        "Podemos comenzar cargando las secuencias que hemos guardado anteriormente. En este caso este paso no sería necesario ya que el proceso de generación de las secuencias es bastante rápido, pero si estamos trabajando con un dataset más grande sí que puede ser conveniente. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "mChmy3q04rJq"
      },
      "outputs": [],
      "source": [
        "in_filename = 'republic_sequences.txt'\n",
        "doc = load_doc(in_filename)\n",
        "lines = doc.split('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Au4s6vM7UED"
      },
      "source": [
        "### Codificando las secuencias\n",
        "\n",
        "Las capas de Embedding esperan que las secuencias de entrada estén compuestas de vectores de enteros. Para ello vamos a identificar cada palabra de nuestro vocabulario con un entero único y codificarlo en una secuencia de entrada. En el futuro cuando vayamos a realizar las predicciones tendremos que realizar el proceso inverso.\n",
        "\n",
        "Para llevar a cabo este proceso de tokenización vamos a usar la API de Keras del siguiente modo. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "eVH8p7Cv52Zo"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "d_odT9JZ710u"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(lines)\n",
        "sequences = tokenizer.texts_to_sequences(lines)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEQoF2S478z9"
      },
      "source": [
        "Ahora podemos acceder a los identificadores de cada palabra usando el atributo ``word_index`` del objeto ``Tokenizer`` que hemos creado. \n",
        "\n",
        "Además debemos determinar el tamaño de nuestro vocabulario para definir la capa de embedding. En concreto, a las palabras de nuestro vocabulario se les han asignado valores entre 1 y el número total de palabras de nuestro vocabulario. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "byu4Vfe075Aj"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaF1yDTF9elO"
      },
      "source": [
        "### Secuencias de entrada y salida\n",
        "\n",
        "Una vez que tenemos codificadas nuestras secuencias tenemos que separarlas en elementos de entrada ($X$) y de salida ($y$). Después de realizar la separación debemos codificar cada palabra usando el método one-hot. Este proceso lo llevaremos a cabo mediante la función ``to_categorical()`` de Keras.\n",
        "Finalmente necesitamos especificar cómo de largas serán las secuencias de entrada. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "SekaGTwl9eCl"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "from numpy import array \n",
        "\n",
        "sequences = array(sequences)\n",
        "X,y=sequences[:,:-1], sequences[:,-1]\n",
        "y = to_categorical(y,num_classes=vocab_size)\n",
        "seq_length = X.shape[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvUMEkkh-gSm"
      },
      "source": [
        "### Entrenando el modelo\n",
        "\n",
        "Ahora podemos definir nuestro modelo que constará de una capa de Embedding, seguida de dos capas LSTM y terminando con una red completamente conectada. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "cs_7VIsS-ODn"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def define_model(vocab_size,seq_length):\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(vocab_size,50,input_length=seq_length))\n",
        "  model.add(LSTM(100,return_sequences=True))\n",
        "  model.add(LSTM(100))\n",
        "  model.add(Dense(100,activation='relu'))\n",
        "  model.add(Dense(vocab_size,activation='softmax'))\n",
        "  model.compile(loss='categorical_crossentropy',optimizer=Adam(),metrics=['accuracy'])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMo-mV9E_cco"
      },
      "source": [
        "Pasamos a entrenar nuestro modelo. Como este proceso es bastante costoso (incluso usando GPUs) en la siguiente sección se proporcionan los ficheros necesarios para usar el modelo. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFxHVuAuCdzC"
      },
      "outputs": [],
      "source": [
        "model = define_model(vocab_size,seq_length)\n",
        "model.fit(X,y,batch_size=128,epochs=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EeUlPGxEifh"
      },
      "source": [
        "Una vez entrenado podemos guardar los pesos del modelo y el tokenizador. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKc5WObR_maO"
      },
      "outputs": [],
      "source": [
        "model.save_weights('./model.h5', overwrite=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kdzX5deoEuiw"
      },
      "outputs": [],
      "source": [
        "from pickle import dump\n",
        "dump(tokenizer, open('tokenizer.pkl','wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rnE1BOhEuIz"
      },
      "source": [
        "## Usando el modelo\n",
        "\n",
        "Como has podido ver en el paso anterior, el proceso de entrenar este tipo de modelos es muy costoso, por lo que puedes descargar los ficheros necesarios para usar el modelo desde el siguiente enlace. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "IJozw5qBFUoO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13e58b51-f144-4bfa-98fb-a616b4a051db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-25 16:15:25--  https://raw.githubusercontent.com/ts1819/datasets/master/practica5/tpu_model.h5\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5101248 (4.9M) [application/octet-stream]\n",
            "Saving to: ‘model.h5’\n",
            "\n",
            "model.h5            100%[===================>]   4.86M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2022-05-25 16:15:25 (117 MB/s) - ‘model.h5’ saved [5101248/5101248]\n",
            "\n",
            "--2022-05-25 16:15:25--  https://raw.githubusercontent.com/ts1819/datasets/master/practica5/tokenizer.pkl\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 353379 (345K) [application/octet-stream]\n",
            "Saving to: ‘tokenizer.pkl’\n",
            "\n",
            "tokenizer.pkl       100%[===================>] 345.10K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2022-05-25 16:15:26 (15.9 MB/s) - ‘tokenizer.pkl’ saved [353379/353379]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/ts1819/datasets/master/practica5/tpu_model.h5 -O model.h5\n",
        "!wget https://raw.githubusercontent.com/ts1819/datasets/master/practica5/tokenizer.pkl -O tokenizer.pkl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tbxpdyp4FZ0t"
      },
      "source": [
        "### Cargando los datos\n",
        "\n",
        "Comenzamos cargando nuestros datos al igual que antes. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "LwREuZMiFfVT"
      },
      "outputs": [],
      "source": [
        "in_filename = 'republic_sequences.txt'\n",
        "doc = load_doc(in_filename)\n",
        "lines = doc.split('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQIu2LzlFl5v"
      },
      "source": [
        "Necesitamos este texto para elegir una secuencia de inicio que será la entrada para nuestro modelo. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "a-IslrBxFlnA"
      },
      "outputs": [],
      "source": [
        "seq_length = len(lines[0].split())-1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejCQcIN_Fxr2"
      },
      "source": [
        "### Cargando el modelo\n",
        "\n",
        "Vamos a cargar el modelo y a fijar los pesos. Notar que para este paso ya no necesitamos el uso de TPU, y que el modelo podría ser usado en cualquier ordenador."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "usmQZBvYF7Q1"
      },
      "outputs": [],
      "source": [
        "model = define_model(vocab_size,seq_length)\n",
        "model.load_weights('./model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uvlOps3GG05"
      },
      "source": [
        "También necesitamos cargar el tokenizador."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "xib_xurDGIpL"
      },
      "outputs": [],
      "source": [
        "from pickle import load\n",
        "tokenizer = load(open('tokenizer.pkl','rb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XrPda8NGUv7"
      },
      "source": [
        "### Generando texto\n",
        "\n",
        "El primer paso para generar el texto consiste en preparar una entrada, para lo cual elegiremos una línea aleatoria del texto. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "1EDkPYM5Gdmz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b66da4e-32d3-4ceb-b0f8-55c80a2de665"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "work of others he sets in order his own inner life and is his own master and his own law and at peace with himself and when he has bound together the three principles within him which may be compared to the higher lower and middle notes of the scale and\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from random import randint\n",
        "seed_text = lines[randint(0,len(lines))]\n",
        "print(seed_text + '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZO-Teo5Go9O"
      },
      "source": [
        "A continuación podemos generar nuevas palabras una por una. Primero, el texto debe codificarse usando el tokenizer que hemos cargado anteriormente. Ahora el modelo puede predecir nuevas palabras usando el método ``predict_classes()`` que devuelve el índice de la palabra con probabilidad más alta. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mNpLQAtHMEY"
      },
      "source": [
        "Esta palabra se añade a nuestro texto inicial y se repite el proceso. Notar que esta secuencia va a ir creciendo por lo que tendremos que truncarla, para lo que utilizamos la función ``pad_sequences()`` de Keras. Todo este proceso se puede implementar con la siguiente función. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "R4J29cLaHZx-"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "def generate_seq(model,tokenizer,seq_length,seed_text,n_words):\n",
        "  result = list()\n",
        "  in_text = seed_text\n",
        "  for _ in range(n_words):\n",
        "    encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
        "    encoded = pad_sequences([encoded],maxlen=seq_length,truncating='pre')\n",
        "    yhat = model.predict(encoded,verbose=0)\n",
        "    yhat=np.argmax(yhat,axis=1)\n",
        "    out_word = ''\n",
        "    for word,index in tokenizer.word_index.items():\n",
        "      if index == yhat:\n",
        "        out_word = word\n",
        "        break\n",
        "    in_text += ' ' + out_word\n",
        "    result.append(out_word)\n",
        "  return ' '.join(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-h6dQKHIIIot"
      },
      "source": [
        "Ahora podemos generar una nueva secuencia usando el siguiente código. Cada vez que lo ejecutemos obtendremos un resultado distinto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "3FUn97EmILPG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80c7ef82-5336-4845-c334-f7ac032612f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "and amid evils such as these will not he who is illgoverned in his own person the tyrannical man i mean whom you just now decided to be the most miserable of all will not he be yet more miserable when instead of leading a private life he is constrained by\n",
            "\n",
            "fortune to be a man to eat over murderers the other creative payment can be a great spirit of revelry not of one step of the other hand is the inference and the other allurements of the same sort and profit to be the just man and the other allurements\n"
          ]
        }
      ],
      "source": [
        "seed_text = lines[randint(0,len(lines))]\n",
        "print(seed_text + '\\n')\n",
        "generated = generate_seq(model,tokenizer,seq_length,seed_text,50)\n",
        "print(generated)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlmSJGumIepK"
      },
      "source": [
        "## Ejercicio\n",
        "\n",
        "Elige tu propio libro del proyecto Gutenberg (es posible usar libros en [español](https://www.gutenberg.org/browse/languages/es)) y crea tu propio modelo de lenguaje. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "v3jsZvvYI21o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5be393f-f9a3-4a36-e490-96e01a943707"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-25 16:24:29--  https://www.gutenberg.org/files/84/84-0.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 448821 (438K) [text/plain]\n",
            "Saving to: ‘frankestein.txt’\n",
            "\n",
            "frankestein.txt     100%[===================>] 438.30K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-05-25 16:24:30 (2.95 MB/s) - ‘frankestein.txt’ saved [448821/448821]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://www.gutenberg.org/files/84/84-0.txt -O frankestein.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head -30 frankestein.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLuJpuRC-6_X",
        "outputId": "a9689c88-08c4-4cb7-8f71-29ee3ffba780"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "﻿The Project Gutenberg eBook of Frankenstein, by Mary Wollstonecraft (Godwin) Shelley\r\n",
            "\r\n",
            "This eBook is for the use of anyone anywhere in the United States and\r\n",
            "most other parts of the world at no cost and with almost no restrictions\r\n",
            "whatsoever. You may copy it, give it away or re-use it under the terms\r\n",
            "of the Project Gutenberg License included with this eBook or online at\r\n",
            "www.gutenberg.org. If you are not located in the United States, you\r\n",
            "will have to check the laws of the country where you are located before\r\n",
            "using this eBook.\r\n",
            "\r\n",
            "Title: Frankenstein\r\n",
            "       or, The Modern Prometheus\r\n",
            "\r\n",
            "Author: Mary Wollstonecraft (Godwin) Shelley\r\n",
            "\r\n",
            "Release Date: 31, 1993 [eBook #84]\r\n",
            "[Most recently updated: November 13, 2020]\r\n",
            "\r\n",
            "Language: English\r\n",
            "\r\n",
            "Character set encoding: UTF-8\r\n",
            "\r\n",
            "Produced by: Judith Boss, Christy Phillips, Lynn Hanninen, and David Meltzer. HTML version by Al Haines.\r\n",
            "Further corrections by Menno de Leeuw.\r\n",
            "\r\n",
            "*** START OF THE PROJECT GUTENBERG EBOOK FRANKENSTEIN ***\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_doc(filename):\n",
        "  # Abrimos el fichero en modo lectura\n",
        "  file = open(filename,'r')\n",
        "  # Leemos el texto completo\n",
        "  text = file.read()\n",
        "  # Cerramos el fichero\n",
        "  file.close()\n",
        "  return text"
      ],
      "metadata": {
        "id": "-KTEIexYASY8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "in_filename = 'frankestein.txt'\n",
        "doc = load_doc(in_filename)"
      ],
      "metadata": {
        "id": "5bLuT9wo_FJu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(doc[:200])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VZYpIa4_MAs",
        "outputId": "4515bac2-001d-448a-cdc6-71ba01d384b3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "﻿The Project Gutenberg eBook of Frankenstein, by Mary Wollstonecraft (Godwin) Shelley\n",
            "\n",
            "This eBook is for the use of anyone anywhere in the United States and\n",
            "most other parts of the world at no cost an\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Limpiando el texto"
      ],
      "metadata": {
        "id": "5oVmSY_xAWWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "\n",
        "def clean_doc(doc):\n",
        "  # Reemplazar '--' con un espacio en blanco ' '\n",
        "  doc = doc.replace('--',' ')\n",
        "  # Partir palabras basándonos en espacios en blanco\n",
        "  tokens = doc.split()\n",
        "  # Vamos a escapar las palabras para poder filtrarlas por caracteres\n",
        "  re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "  # Eliminamos los símbolos de puntuación\n",
        "  tokens = [re_punc.sub('',w) for w in tokens]\n",
        "  # Eliminamos elementos que nos son alfabéticos\n",
        "  tokens = [word for word in tokens if word.isalpha()]\n",
        "  # Convertimos a minúsculas\n",
        "  tokens = [word.lower() for word in tokens]\n",
        "  return tokens\n",
        "  "
      ],
      "metadata": {
        "id": "K-ZcvTIHAXwh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = clean_doc(doc)\n",
        "print(tokens[:200])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TGtmsRn_PKl",
        "outputId": "28049be4-f43d-44cc-a20d-69cc677e7579"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['project', 'gutenberg', 'ebook', 'of', 'frankenstein', 'by', 'mary', 'wollstonecraft', 'godwin', 'shelley', 'this', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in', 'the', 'united', 'states', 'and', 'most', 'other', 'parts', 'of', 'the', 'world', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', 'you', 'may', 'copy', 'it', 'give', 'it', 'away', 'or', 'reuse', 'it', 'under', 'the', 'terms', 'of', 'the', 'project', 'gutenberg', 'license', 'included', 'with', 'this', 'ebook', 'or', 'online', 'at', 'wwwgutenbergorg', 'if', 'you', 'are', 'not', 'located', 'in', 'the', 'united', 'states', 'you', 'will', 'have', 'to', 'check', 'the', 'laws', 'of', 'the', 'country', 'where', 'you', 'are', 'located', 'before', 'using', 'this', 'ebook', 'title', 'frankenstein', 'or', 'the', 'modern', 'prometheus', 'author', 'mary', 'wollstonecraft', 'godwin', 'shelley', 'release', 'date', 'ebook', 'most', 'recently', 'updated', 'november', 'language', 'english', 'character', 'set', 'encoding', 'produced', 'by', 'judith', 'boss', 'christy', 'phillips', 'lynn', 'hanninen', 'and', 'david', 'meltzer', 'html', 'version', 'by', 'al', 'haines', 'further', 'corrections', 'by', 'menno', 'de', 'leeuw', 'start', 'of', 'the', 'project', 'gutenberg', 'ebook', 'frankenstein', 'frankenstein', 'or', 'the', 'modern', 'prometheus', 'by', 'mary', 'wollstonecraft', 'godwin', 'shelley', 'contents', 'letter', 'letter', 'letter', 'letter', 'chapter', 'chapter', 'chapter', 'chapter', 'chapter', 'chapter', 'chapter', 'chapter', 'chapter', 'chapter', 'chapter', 'chapter', 'chapter', 'chapter', 'chapter', 'chapter', 'chapter', 'chapter', 'chapter', 'chapter', 'chapter', 'chapter', 'chapter', 'chapter', 'letter', 'to', 'mrs', 'saville', 'england', 'st', 'petersburgh', 'dec', 'you', 'will', 'rejoice', 'to', 'hear', 'that', 'no', 'disaster', 'has']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Total Tokens: %d' % len(tokens))\n",
        "print('Unique Tokens: %d' %len(set(tokens)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5y-ubW8N_REX",
        "outputId": "bfdb43e9-93ad-41f8-f60b-5e1e50b4a1e6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Tokens: 76924\n",
            "Unique Tokens: 7262\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Guardando el texto limpio"
      ],
      "metadata": {
        "id": "iC5FTfvuAfaN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def organize_tokens(tokens,input_len=50,output_len=1):\n",
        "  length = input_len + output_len\n",
        "  sequences = list()\n",
        "  for i in range(length,len(tokens)):\n",
        "    # Elegimos la secuencia de tokens\n",
        "    seq = tokens[i-length:i]\n",
        "    # Convertimos la secuencia en una línea\n",
        "    line = ' '.join(seq)\n",
        "    # Almacenamos el resultado\n",
        "    sequences.append(line)\n",
        "  return sequences"
      ],
      "metadata": {
        "id": "t4ou3HYyAkam"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lines = organize_tokens(tokens)"
      ],
      "metadata": {
        "id": "PXduaCeM_R1l"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_doc(lines,filename):\n",
        "  data = '\\n'.join(lines)\n",
        "  file = open(filename,'w')\n",
        "  file.write(data)\n",
        "  file.close()"
      ],
      "metadata": {
        "id": "Z508Ir0ZAmHY"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_doc(lines,'frankestein_sequences.txt')"
      ],
      "metadata": {
        "id": "3MnA407t_bXq"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head -5 frankestein_sequences.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNj6mTf8_haJ",
        "outputId": "c8401e39-cf35-40e4-d939-a4a1603325cb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "project gutenberg ebook of frankenstein by mary wollstonecraft godwin shelley this ebook is for the use of anyone anywhere in the united states and most other parts of the world at no cost and with almost no restrictions whatsoever you may copy it give it away or reuse it under the\n",
            "gutenberg ebook of frankenstein by mary wollstonecraft godwin shelley this ebook is for the use of anyone anywhere in the united states and most other parts of the world at no cost and with almost no restrictions whatsoever you may copy it give it away or reuse it under the terms\n",
            "ebook of frankenstein by mary wollstonecraft godwin shelley this ebook is for the use of anyone anywhere in the united states and most other parts of the world at no cost and with almost no restrictions whatsoever you may copy it give it away or reuse it under the terms of\n",
            "of frankenstein by mary wollstonecraft godwin shelley this ebook is for the use of anyone anywhere in the united states and most other parts of the world at no cost and with almost no restrictions whatsoever you may copy it give it away or reuse it under the terms of the\n",
            "frankenstein by mary wollstonecraft godwin shelley this ebook is for the use of anyone anywhere in the united states and most other parts of the world at no cost and with almost no restrictions whatsoever you may copy it give it away or reuse it under the terms of the project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entrenar modelo"
      ],
      "metadata": {
        "id": "iopXE9leAo4a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = load_doc('frankestein_sequences.txt')\n",
        "lines = doc.split('\\n')"
      ],
      "metadata": {
        "id": "bJYsAYe9_j0_"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Codificacion de las secuencias"
      ],
      "metadata": {
        "id": "c0M3JI9GAvCi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "metadata": {
        "id": "ML-ThBPhAunw"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(lines)\n",
        "sequences = tokenizer.texts_to_sequences(lines)"
      ],
      "metadata": {
        "id": "kGVbyO-f_paj"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1 "
      ],
      "metadata": {
        "id": "YuzVIQRK_s6R"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Separación de secuencias en entrada y salida"
      ],
      "metadata": {
        "id": "0I1XekrZA0EC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "from numpy import array \n",
        "\n",
        "sequences = array(sequences)\n",
        "X,y=sequences[:,:-1], sequences[:,-1]\n",
        "y = to_categorical(y,num_classes=vocab_size)\n",
        "seq_length = X.shape[1]"
      ],
      "metadata": {
        "id": "FYCfpyK5_vVE"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def define_model(vocab_size,seq_length):\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(vocab_size,50,input_length=seq_length))\n",
        "  model.add(LSTM(100,return_sequences=True))\n",
        "  model.add(LSTM(100))\n",
        "  model.add(Dense(100,activation='relu'))\n",
        "  model.add(Dense(vocab_size,activation='softmax'))\n",
        "  model.compile(loss='categorical_crossentropy',optimizer=Adam(),metrics=['accuracy'])\n",
        "  return model"
      ],
      "metadata": {
        "id": "GFEokZVRB9iv"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = define_model(vocab_size,seq_length)\n",
        "model.fit(X,y,batch_size=128,epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKMJdzBYB_RF",
        "outputId": "dfa38228-71a8-4d31-c341-24b9a5ea5b4e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "601/601 [==============================] - 16s 14ms/step - loss: 6.6364 - accuracy: 0.0597\n",
            "Epoch 2/100\n",
            "601/601 [==============================] - 9s 15ms/step - loss: 6.1738 - accuracy: 0.0806\n",
            "Epoch 3/100\n",
            "601/601 [==============================] - 9s 15ms/step - loss: 5.9754 - accuracy: 0.0972\n",
            "Epoch 4/100\n",
            "601/601 [==============================] - 10s 17ms/step - loss: 5.8034 - accuracy: 0.1116\n",
            "Epoch 5/100\n",
            "601/601 [==============================] - 10s 16ms/step - loss: 5.6541 - accuracy: 0.1226\n",
            "Epoch 6/100\n",
            "601/601 [==============================] - 9s 15ms/step - loss: 5.5333 - accuracy: 0.1300\n",
            "Epoch 7/100\n",
            "601/601 [==============================] - 9s 15ms/step - loss: 5.4272 - accuracy: 0.1375\n",
            "Epoch 8/100\n",
            "601/601 [==============================] - 9s 15ms/step - loss: 5.3379 - accuracy: 0.1425\n",
            "Epoch 9/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 5.2494 - accuracy: 0.1464\n",
            "Epoch 10/100\n",
            "601/601 [==============================] - 9s 15ms/step - loss: 5.1698 - accuracy: 0.1492\n",
            "Epoch 11/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 5.0947 - accuracy: 0.1527\n",
            "Epoch 12/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 5.0217 - accuracy: 0.1556\n",
            "Epoch 13/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 4.9524 - accuracy: 0.1575\n",
            "Epoch 14/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 4.8865 - accuracy: 0.1598\n",
            "Epoch 15/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 4.8194 - accuracy: 0.1626\n",
            "Epoch 16/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 4.7588 - accuracy: 0.1638\n",
            "Epoch 17/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 4.6981 - accuracy: 0.1667\n",
            "Epoch 18/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 4.6392 - accuracy: 0.1679\n",
            "Epoch 19/100\n",
            "601/601 [==============================] - 9s 15ms/step - loss: 4.5810 - accuracy: 0.1709\n",
            "Epoch 20/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 4.5265 - accuracy: 0.1719\n",
            "Epoch 21/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 4.4720 - accuracy: 0.1747\n",
            "Epoch 22/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 4.4199 - accuracy: 0.1778\n",
            "Epoch 23/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 4.3689 - accuracy: 0.1791\n",
            "Epoch 24/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 4.3224 - accuracy: 0.1817\n",
            "Epoch 25/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 4.2745 - accuracy: 0.1850\n",
            "Epoch 26/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 4.2319 - accuracy: 0.1888\n",
            "Epoch 27/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 4.1890 - accuracy: 0.1909\n",
            "Epoch 28/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 4.1491 - accuracy: 0.1935\n",
            "Epoch 29/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 4.1109 - accuracy: 0.1972\n",
            "Epoch 30/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 4.0719 - accuracy: 0.2019\n",
            "Epoch 31/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 4.0351 - accuracy: 0.2045\n",
            "Epoch 32/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 4.0035 - accuracy: 0.2073\n",
            "Epoch 33/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 3.9650 - accuracy: 0.2116\n",
            "Epoch 34/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 3.9340 - accuracy: 0.2144\n",
            "Epoch 35/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 3.9027 - accuracy: 0.2184\n",
            "Epoch 36/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 3.8707 - accuracy: 0.2207\n",
            "Epoch 37/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 3.8407 - accuracy: 0.2259\n",
            "Epoch 38/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 3.8099 - accuracy: 0.2290\n",
            "Epoch 39/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 3.7801 - accuracy: 0.2322\n",
            "Epoch 40/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 3.7527 - accuracy: 0.2350\n",
            "Epoch 41/100\n",
            "601/601 [==============================] - 9s 15ms/step - loss: 3.7234 - accuracy: 0.2393\n",
            "Epoch 42/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 3.6957 - accuracy: 0.2429\n",
            "Epoch 43/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 3.6679 - accuracy: 0.2460\n",
            "Epoch 44/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 3.6433 - accuracy: 0.2484\n",
            "Epoch 45/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 3.6199 - accuracy: 0.2524\n",
            "Epoch 46/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 3.5919 - accuracy: 0.2571\n",
            "Epoch 47/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 3.5642 - accuracy: 0.2606\n",
            "Epoch 48/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 3.5395 - accuracy: 0.2624\n",
            "Epoch 49/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 3.5157 - accuracy: 0.2657\n",
            "Epoch 50/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 3.4946 - accuracy: 0.2696\n",
            "Epoch 51/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 3.4676 - accuracy: 0.2735\n",
            "Epoch 52/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 3.4441 - accuracy: 0.2764\n",
            "Epoch 53/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 3.4176 - accuracy: 0.2797\n",
            "Epoch 54/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 3.4001 - accuracy: 0.2823\n",
            "Epoch 55/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 3.3770 - accuracy: 0.2860\n",
            "Epoch 56/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 3.3546 - accuracy: 0.2889\n",
            "Epoch 57/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 3.3311 - accuracy: 0.2926\n",
            "Epoch 58/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 3.3097 - accuracy: 0.2947\n",
            "Epoch 59/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 3.2890 - accuracy: 0.2980\n",
            "Epoch 60/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 3.2648 - accuracy: 0.3026\n",
            "Epoch 61/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 3.2443 - accuracy: 0.3064\n",
            "Epoch 62/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 3.2259 - accuracy: 0.3080\n",
            "Epoch 63/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 3.2036 - accuracy: 0.3114\n",
            "Epoch 64/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 3.1843 - accuracy: 0.3136\n",
            "Epoch 65/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 3.1669 - accuracy: 0.3169\n",
            "Epoch 66/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 3.1439 - accuracy: 0.3210\n",
            "Epoch 67/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 3.1254 - accuracy: 0.3234\n",
            "Epoch 68/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 3.1038 - accuracy: 0.3271\n",
            "Epoch 69/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 3.0849 - accuracy: 0.3304\n",
            "Epoch 70/100\n",
            "601/601 [==============================] - 9s 15ms/step - loss: 3.0693 - accuracy: 0.3326\n",
            "Epoch 71/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 3.0477 - accuracy: 0.3351\n",
            "Epoch 72/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 3.0289 - accuracy: 0.3375\n",
            "Epoch 73/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 3.0152 - accuracy: 0.3410\n",
            "Epoch 74/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 2.9927 - accuracy: 0.3443\n",
            "Epoch 75/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 2.9746 - accuracy: 0.3486\n",
            "Epoch 76/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 2.9586 - accuracy: 0.3495\n",
            "Epoch 77/100\n",
            "601/601 [==============================] - 9s 15ms/step - loss: 2.9402 - accuracy: 0.3539\n",
            "Epoch 78/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 2.9203 - accuracy: 0.3573\n",
            "Epoch 79/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 2.9052 - accuracy: 0.3595\n",
            "Epoch 80/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 2.8886 - accuracy: 0.3632\n",
            "Epoch 81/100\n",
            "601/601 [==============================] - 8s 14ms/step - loss: 2.8691 - accuracy: 0.3658\n",
            "Epoch 82/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 2.8543 - accuracy: 0.3690\n",
            "Epoch 83/100\n",
            "601/601 [==============================] - 8s 14ms/step - loss: 2.8363 - accuracy: 0.3721\n",
            "Epoch 84/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 2.8221 - accuracy: 0.3742\n",
            "Epoch 85/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 2.8037 - accuracy: 0.3765\n",
            "Epoch 86/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 2.7868 - accuracy: 0.3782\n",
            "Epoch 87/100\n",
            "601/601 [==============================] - 9s 15ms/step - loss: 2.7706 - accuracy: 0.3814\n",
            "Epoch 88/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 2.7552 - accuracy: 0.3861\n",
            "Epoch 89/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 2.7387 - accuracy: 0.3881\n",
            "Epoch 90/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 2.7245 - accuracy: 0.3915\n",
            "Epoch 91/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 2.7091 - accuracy: 0.3934\n",
            "Epoch 92/100\n",
            "601/601 [==============================] - 8s 14ms/step - loss: 2.6931 - accuracy: 0.3970\n",
            "Epoch 93/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 2.6745 - accuracy: 0.3994\n",
            "Epoch 94/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 2.6603 - accuracy: 0.4024\n",
            "Epoch 95/100\n",
            "601/601 [==============================] - 9s 15ms/step - loss: 2.6438 - accuracy: 0.4062\n",
            "Epoch 96/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 2.6331 - accuracy: 0.4101\n",
            "Epoch 97/100\n",
            "601/601 [==============================] - 9s 15ms/step - loss: 2.6148 - accuracy: 0.4105\n",
            "Epoch 98/100\n",
            "601/601 [==============================] - 9s 15ms/step - loss: 2.6019 - accuracy: 0.4126\n",
            "Epoch 99/100\n",
            "601/601 [==============================] - 9s 15ms/step - loss: 2.5838 - accuracy: 0.4171\n",
            "Epoch 100/100\n",
            "601/601 [==============================] - 9s 14ms/step - loss: 2.5710 - accuracy: 0.4202\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7417a34d90>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cargando datos"
      ],
      "metadata": {
        "id": "RUW4lTZABKI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = load_doc('frankestein_sequences.txt')\n",
        "lines = doc.split('\\n')"
      ],
      "metadata": {
        "id": "lByvqvM1BM3A"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length1 = len(lines[0].split())-1"
      ],
      "metadata": {
        "id": "1Mt3MkJhBXCz"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generando texto"
      ],
      "metadata": {
        "id": "QRk33uY1CrJu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from random import randint\n",
        "seed_text = lines[randint(0,len(lines))]\n",
        "print(seed_text + '\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sS83QLU_CtM4",
        "outputId": "f3c8f2bc-5af9-47c6-c68c-19ae397a260b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "met me at midnight among the precipices of an inaccessible mountain i remembered also the nervous fever with which i had been seized just at the time that i dated my creation and which would give an air of delirium to a tale otherwise so utterly improbable i well knew that\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "def generate_seq(model,tokenizer,seq_length,seed_text,n_words):\n",
        "  result = list()\n",
        "  in_text = seed_text\n",
        "  for _ in range(n_words):\n",
        "    encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
        "    encoded = pad_sequences([encoded],maxlen=seq_length,truncating='pre')\n",
        "    yhat = model.predict(encoded,verbose=0)\n",
        "    yhat=np.argmax(yhat,axis=1)\n",
        "    out_word = ''\n",
        "    for word,index in tokenizer.word_index.items():\n",
        "      if index == yhat:\n",
        "        out_word = word\n",
        "        break\n",
        "    in_text += ' ' + out_word\n",
        "    result.append(out_word)\n",
        "  return ' '.join(result)"
      ],
      "metadata": {
        "id": "F9Q_yQx5CucI"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed_text = lines[randint(0,len(lines))]\n",
        "print(seed_text + '\\n')\n",
        "generated = generate_seq(model,tokenizer,seq_length,seed_text,50)\n",
        "print(generated)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXxXjO_yCvoT",
        "outputId": "1d83aa8a-c241-463c-8044-2d62800f60ad"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "melancholy sweetness i see his thin hand raised in animation while the lineaments of his face are irradiated by the soul within strange and harrowing must be his story frightful the storm which embraced the gallant vessel on its course and wrecked chapter i am by birth a genevese and my\n",
            "\n",
            "father was enraptured and once in existence and i was innocent i was destined to endure the slightest shadow of english poured and prescribed generations to descend the remainder of the fiend i had been out of a pail and at the bottom of the dusky plain i strained my\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SG188-6TI3ph"
      },
      "source": [
        "Recuerda guardar este notebook en tu repositorio usando la opción \"Save in GitHub\" del menú File."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "practica9_3.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}