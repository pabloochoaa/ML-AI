{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IA2122/practica-9-paochoa/blob/main/practica9_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6phqqXPIv6hp"
      },
      "source": [
        "# Práctica 9 Parte 1: Word embeddings\n",
        "\n",
        "En este primer notebook veremos distintos word embeddings. Para ello usaremos una librería de procesamiento de lenguaje natural llamada [gensim](https://radimrehurek.com/gensim/index.html). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Javw4eadeH9B"
      },
      "source": [
        "## Modelo antiguo\n",
        "\n",
        "Antes de hablar de los modelos modernos, conviene conocer los modelos antiguos y sus limitaciones. \n",
        "\n",
        "Una de las técnicas más utilizadas hasta hace poco para codificar frases era conocida como [bolsa de palabras (o *bag of words*)](https://en.wikipedia.org/wiki/Bag-of-words_model). Esta técnica transforma cada documento a un vector de enteros de longitud fija. \n",
        "\n",
        "Por ejemplo, para las siguientes dos frases:\n",
        "\n",
        "1. A Juan le gusta ver películas. A María también le gusta.\n",
        "2. A Juan le gusta ver partidos de fútbol. María odia el futbol. \n",
        "\n",
        "El modelo produce los siguientes vectores (cada fila de la siguiente tabla representa una de las frases).\n",
        "\n",
        "\n",
        "| Palabras | A | Juan | le | gusta | ver | películas | María | también | partidos | fútbol | odia | el |\n",
        "|---------|--------- | --------- | --------- | --------- | --------- | --------- | --------- | --------- | --------- | --------- | --------- | --------- |\n",
        "|Frase 1|2 | 1 | 2 | 1 | 1 | 1 | 1 |1 | 0 | 0 | 0 | 0 |\n",
        "|Frase 2|1 | 1 | 1 | 1 | 1 | 0 | 1 |0 | 1 | 1 | 1 | 1 |\n",
        "\n",
        "Cada vector tiene 12 elementos, donde cada elemento cuenta el número de veces que una determinada palabra ocurre en dicho documento. \n",
        "\n",
        "Este modelo era muy efectivo, pero tenía varias limitaciones. En primer lugar se pierde información sobre el orden de las palabras. Por ejemplo, las frases \"A María le gusta Juan\" y \"A Juan le gusta María\" tienen vectores idénticos. La solución consiste en usar [n-gramas](https://en.wikipedia.org/wiki/N-gram). Además este modelo tiene la limitación de ser *sparse* (muchos ceros para cada vector) y tener una dimensionalidad muy alta. \n",
        "\n",
        "Otro de los problemas más importantes es que este modelo no aprende el significado de las palabras subyacentes, y como consecuencia la distancia entre vectores no refleja la similitud o diferencia en significado. Estos problemas se han resuelto con los word-embeddings.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KIl_YtyhYY_"
      },
      "source": [
        "## Word2Vec\n",
        "\n",
        "El modelo [Word2Vec](https://arxiv.org/pdf/1301.3781.pdf) fue introducido en 2013 por investigadores de Google, y es capaz de incrustar las palabras en un vector de dimensión bja usando una red neuronal. El resultado es un modelo donde los vectores que están cerca en el espacio tienen significados similares basados en el contexto. Es decir, no existe un único modelo word2vec, sino que es un modelo que se entrena a partir de un dataset (dicho dataset puede ser la wikipedia, twitter, ...).\n",
        "\n",
        "Vamos a ver que es posible hacer con uno de estos modelos. Para ello vamos a usar un modelo entrenado en el dataset de noticias de Google. Por el momento vamos a usar modelos entrenados en inglés, más adelante en la práctica veremos cómo usar modelos en español. \n",
        "\n",
        "Comenzamos descargando el dataset (esto puede costar un tiempo). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0BkZ1wzczrx"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader as api\n",
        "wv = api.load('word2vec-google-news-300')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3tvMtbNjelh"
      },
      "source": [
        "Una tarea común es obtener las primeras palabras del vocabulario usado para construir el modelo. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_itVhwNjfLT",
        "outputId": "bc543508-2d17-4908-81f6-727522a4d35a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "</s>\n",
            "in\n",
            "for\n",
            "that\n",
            "is\n",
            "on\n",
            "##\n",
            "The\n",
            "with\n",
            "said\n"
          ]
        }
      ],
      "source": [
        "for i, word in enumerate(wv.vocab):\n",
        "    if i == 10:\n",
        "        break\n",
        "    print(word)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1t_LCqWjuMZ"
      },
      "source": [
        "Ahora podemos ver cuál es la representación de una palabra. \n",
        "\n",
        "Su representación no es de las 4 letras que la componen sólo, si no de su contexto y todo lo que acarrea una palabra."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aAgc4y3jgAs",
        "outputId": "99947a26-3087-4807-90ab-f5b202f008cb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1.25976562e-01,  2.97851562e-02,  8.60595703e-03,  1.39648438e-01,\n",
              "       -2.56347656e-02, -3.61328125e-02,  1.11816406e-01, -1.98242188e-01,\n",
              "        5.12695312e-02,  3.63281250e-01, -2.42187500e-01, -3.02734375e-01,\n",
              "       -1.77734375e-01, -2.49023438e-02, -1.67968750e-01, -1.69921875e-01,\n",
              "        3.46679688e-02,  5.21850586e-03,  4.63867188e-02,  1.28906250e-01,\n",
              "        1.36718750e-01,  1.12792969e-01,  5.95703125e-02,  1.36718750e-01,\n",
              "        1.01074219e-01, -1.76757812e-01, -2.51953125e-01,  5.98144531e-02,\n",
              "        3.41796875e-01, -3.11279297e-02,  1.04492188e-01,  6.17675781e-02,\n",
              "        1.24511719e-01,  4.00390625e-01, -3.22265625e-01,  8.39843750e-02,\n",
              "        3.90625000e-02,  5.85937500e-03,  7.03125000e-02,  1.72851562e-01,\n",
              "        1.38671875e-01, -2.31445312e-01,  2.83203125e-01,  1.42578125e-01,\n",
              "        3.41796875e-01, -2.39257812e-02, -1.09863281e-01,  3.32031250e-02,\n",
              "       -5.46875000e-02,  1.53198242e-02, -1.62109375e-01,  1.58203125e-01,\n",
              "       -2.59765625e-01,  2.01416016e-02, -1.63085938e-01,  1.35803223e-03,\n",
              "       -1.44531250e-01, -5.68847656e-02,  4.29687500e-02, -2.46582031e-02,\n",
              "        1.85546875e-01,  4.47265625e-01,  9.58251953e-03,  1.31835938e-01,\n",
              "        9.86328125e-02, -1.85546875e-01, -1.00097656e-01, -1.33789062e-01,\n",
              "       -1.25000000e-01,  2.83203125e-01,  1.23046875e-01,  5.32226562e-02,\n",
              "       -1.77734375e-01,  8.59375000e-02, -2.18505859e-02,  2.05078125e-02,\n",
              "       -1.39648438e-01,  2.51464844e-02,  1.38671875e-01, -1.05468750e-01,\n",
              "        1.38671875e-01,  8.88671875e-02, -7.51953125e-02, -2.13623047e-02,\n",
              "        1.72851562e-01,  4.63867188e-02, -2.65625000e-01,  8.91113281e-03,\n",
              "        1.49414062e-01,  3.78417969e-02,  2.38281250e-01, -1.24511719e-01,\n",
              "       -2.17773438e-01, -1.81640625e-01,  2.97851562e-02,  5.71289062e-02,\n",
              "       -2.89306641e-02,  1.24511719e-02,  9.66796875e-02, -2.31445312e-01,\n",
              "        5.81054688e-02,  6.68945312e-02,  7.08007812e-02, -3.08593750e-01,\n",
              "       -2.14843750e-01,  1.45507812e-01, -4.27734375e-01, -9.39941406e-03,\n",
              "        1.54296875e-01, -7.66601562e-02,  2.89062500e-01,  2.77343750e-01,\n",
              "       -4.86373901e-04, -1.36718750e-01,  3.24218750e-01, -2.46093750e-01,\n",
              "       -3.03649902e-03, -2.11914062e-01,  1.25000000e-01,  2.69531250e-01,\n",
              "        2.04101562e-01,  8.25195312e-02, -2.01171875e-01, -1.60156250e-01,\n",
              "       -3.78417969e-02, -1.20117188e-01,  1.15234375e-01, -4.10156250e-02,\n",
              "       -3.95507812e-02, -8.98437500e-02,  6.34765625e-03,  2.03125000e-01,\n",
              "        1.86523438e-01,  2.73437500e-01,  6.29882812e-02,  1.41601562e-01,\n",
              "       -9.81445312e-02,  1.38671875e-01,  1.82617188e-01,  1.73828125e-01,\n",
              "        1.73828125e-01, -2.37304688e-01,  1.78710938e-01,  6.34765625e-02,\n",
              "        2.36328125e-01, -2.08984375e-01,  8.74023438e-02, -1.66015625e-01,\n",
              "       -7.91015625e-02,  2.43164062e-01, -8.88671875e-02,  1.26953125e-01,\n",
              "       -2.16796875e-01, -1.73828125e-01, -3.59375000e-01, -8.25195312e-02,\n",
              "       -6.49414062e-02,  5.07812500e-02,  1.35742188e-01, -7.47070312e-02,\n",
              "       -1.64062500e-01,  1.15356445e-02,  4.45312500e-01, -2.15820312e-01,\n",
              "       -1.11328125e-01, -1.92382812e-01,  1.70898438e-01, -1.25000000e-01,\n",
              "        2.65502930e-03,  1.92382812e-01, -1.74804688e-01,  1.39648438e-01,\n",
              "        2.92968750e-01,  1.13281250e-01,  5.95703125e-02, -6.39648438e-02,\n",
              "        9.96093750e-02, -2.72216797e-02,  1.96533203e-02,  4.27246094e-02,\n",
              "       -2.46093750e-01,  6.39648438e-02, -2.25585938e-01, -1.68945312e-01,\n",
              "        2.89916992e-03,  8.20312500e-02,  3.41796875e-01,  4.32128906e-02,\n",
              "        1.32812500e-01,  1.42578125e-01,  7.61718750e-02,  5.98144531e-02,\n",
              "       -1.19140625e-01,  2.74658203e-03, -6.29882812e-02, -2.72216797e-02,\n",
              "       -4.82177734e-03, -8.20312500e-02, -2.49023438e-02, -4.00390625e-01,\n",
              "       -1.06933594e-01,  4.24804688e-02,  7.76367188e-02, -1.16699219e-01,\n",
              "        7.37304688e-02, -9.22851562e-02,  1.07910156e-01,  1.58203125e-01,\n",
              "        4.24804688e-02,  1.26953125e-01,  3.61328125e-02,  2.67578125e-01,\n",
              "       -1.01074219e-01, -3.02734375e-01, -5.76171875e-02,  5.05371094e-02,\n",
              "        5.26428223e-04, -2.07031250e-01, -1.38671875e-01, -8.97216797e-03,\n",
              "       -2.78320312e-02, -1.41601562e-01,  2.07031250e-01, -1.58203125e-01,\n",
              "        1.27929688e-01,  1.49414062e-01, -2.24609375e-02, -8.44726562e-02,\n",
              "        1.22558594e-01,  2.15820312e-01, -2.13867188e-01, -3.12500000e-01,\n",
              "       -3.73046875e-01,  4.08935547e-03,  1.07421875e-01,  1.06933594e-01,\n",
              "        7.32421875e-02,  8.97216797e-03, -3.88183594e-02, -1.29882812e-01,\n",
              "        1.49414062e-01, -2.14843750e-01, -1.83868408e-03,  9.91210938e-02,\n",
              "        1.57226562e-01, -1.14257812e-01, -2.05078125e-01,  9.91210938e-02,\n",
              "        3.69140625e-01, -1.97265625e-01,  3.54003906e-02,  1.09375000e-01,\n",
              "        1.31835938e-01,  1.66992188e-01,  2.35351562e-01,  1.04980469e-01,\n",
              "       -4.96093750e-01, -1.64062500e-01, -1.56250000e-01, -5.22460938e-02,\n",
              "        1.03027344e-01,  2.43164062e-01, -1.88476562e-01,  5.07812500e-02,\n",
              "       -9.37500000e-02, -6.68945312e-02,  2.27050781e-02,  7.61718750e-02,\n",
              "        2.89062500e-01,  3.10546875e-01, -5.37109375e-02,  2.28515625e-01,\n",
              "        2.51464844e-02,  6.78710938e-02, -1.21093750e-01, -2.15820312e-01,\n",
              "       -2.73437500e-01, -3.07617188e-02, -3.37890625e-01,  1.53320312e-01,\n",
              "        2.33398438e-01, -2.08007812e-01,  3.73046875e-01,  8.20312500e-02,\n",
              "        2.51953125e-01, -7.61718750e-02, -4.66308594e-02, -2.23388672e-02,\n",
              "        2.99072266e-02, -5.93261719e-02, -4.66918945e-03, -2.44140625e-01,\n",
              "       -2.09960938e-01, -2.87109375e-01, -4.54101562e-02, -1.77734375e-01,\n",
              "       -2.79296875e-01, -8.59375000e-02,  9.13085938e-02,  2.51953125e-01],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "wv['king']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVkz-1Otjz66"
      },
      "source": [
        "Desafortunadamente el modelo no es capaz de inferir el vector asociado a palabras \"raras\". "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VqAUgedWjf8w",
        "outputId": "4e80dc11-21c0-4bab-8029-73753255b093"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The word 'cameroon' does not appear in this model\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    vec_cameroon = wv['cameroon']\n",
        "except KeyError:\n",
        "    print(\"The word 'cameroon' does not appear in this model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojzyAEIFkByR"
      },
      "source": [
        "Estos modelos proporcionan distintas funciones para tratar varios problemas. \n",
        "\n",
        "Por ejemplo, podemos ver la similaridad entre palabras. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsWrXJurjf6E",
        "outputId": "7a39645d-c855-4bae-aefc-c1eb085bc5b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'car'\t'minivan'\t0.69\n",
            "'car'\t'bicycle'\t0.54\n",
            "'car'\t'airplane'\t0.42\n",
            "'car'\t'cereal'\t0.14\n",
            "'car'\t'communism'\t0.06\n"
          ]
        }
      ],
      "source": [
        "pairs = [\n",
        "    ('car', 'minivan'),   # a minivan is a kind of car\n",
        "    ('car', 'bicycle'),   # still a wheeled vehicle\n",
        "    ('car', 'airplane'),  # ok, no wheels, but still a vehicle\n",
        "    ('car', 'cereal'),    # ... and so on\n",
        "    ('car', 'communism'),\n",
        "]\n",
        "for w1, w2 in pairs:\n",
        "    print('%r\\t%r\\t%.2f' % (w1, w2, wv.similarity(w1, w2)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufrSR7o3kYEu"
      },
      "source": [
        "Podemos también mostrar las 5 palabras más similares a *car*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cj6mSQZyjf3b",
        "outputId": "28fe4ce2-1abe-4760-e976-272988188068"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('vehicle', 0.7821096181869507), ('cars', 0.7423830032348633), ('SUV', 0.7160962820053101), ('minivan', 0.6907036304473877), ('truck', 0.6735789775848389)]\n"
          ]
        }
      ],
      "source": [
        "print(wv.most_similar(positive=['car'], topn=5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImwXvhOZkq8a"
      },
      "source": [
        "**Ejercicio**: Busca las 5 palabras más similares para Spain. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TeTQfrdjfxH",
        "outputId": "030e03c1-aa58-45a0-a6b2-e377dc9188be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Portugal', 0.7220357656478882), ('Inveravante_Inversiones_SL', 0.6925067901611328), ('Spains', 0.6856307983398438), ('Madrid', 0.6743447780609131), ('Spaniards', 0.6629219651222229)]\n"
          ]
        }
      ],
      "source": [
        "print(wv.most_similar(positive=['Spain'], topn=5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6WemyjKlaEd"
      },
      "source": [
        "Es posible también buscar analogías. Por ejemplo, para resolver la analogía *man is to king, as woman is to ...*, se debe ejecutar la siguiente instrucción."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KeLl2HSl6Bi",
        "outputId": "74cdc11c-9f4e-4d52-beeb-9f4a62a8bf5c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('queen', 0.9314123392105103),\n",
              " ('monarch', 0.858533501625061),\n",
              " ('princess', 0.8476566076278687),\n",
              " ('Queen_Consort', 0.8150269985198975),\n",
              " ('queens', 0.8099815249443054),\n",
              " ('crown_prince', 0.808997631072998),\n",
              " ('royal_palace', 0.8027306795120239),\n",
              " ('monarchy', 0.801961362361908),\n",
              " ('prince', 0.800979733467102),\n",
              " ('empress', 0.7958388328552246)]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "wv.most_similar_cosmul(positive=['king','woman'],negative=['man'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oR5wgd21mIcd"
      },
      "source": [
        "**Ejercicio**: Da respuesta a las siguientes analogías. \n",
        "\n",
        "*Eat is to ate, as go is to ...*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iL2ufFm-jfmL",
        "outputId": "67b9960c-cd6d-4bdd-c05d-86384b1773d8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('went', 0.9165000319480896),\n",
              " ('gone', 0.8448726534843445),\n",
              " ('came', 0.816146969795227),\n",
              " ('ran', 0.8127626776695251),\n",
              " ('stayed', 0.7917163372039795),\n",
              " ('goes', 0.7796573042869568),\n",
              " ('sneaked', 0.774756669998169),\n",
              " ('snuck', 0.768629252910614),\n",
              " ('got', 0.7652180790901184),\n",
              " ('walked', 0.7639439702033997)]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "wv.most_similar_cosmul(positive=['ate','go'],negative=['eat'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xlw0iydonBdu"
      },
      "source": [
        "*Madrid is to Spain, as Berlin is to ...*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yrmahqHnBBC",
        "outputId": "e5e91295-5f53-48a2-8547-48529a00a3bf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Germany', 0.9708642959594727),\n",
              " ('Austria', 0.8568971753120422),\n",
              " ('German', 0.8524766564369202),\n",
              " ('Hungary', 0.8441289663314819),\n",
              " ('Poland', 0.8382856845855713),\n",
              " ('Annita_Kirsten', 0.8365224599838257),\n",
              " ('Thielert_AG_Hamburg', 0.8243475556373596),\n",
              " ('Buffalo_Sabres_Jochen_Hecht', 0.8205474019050598),\n",
              " ('symbol_RSTI', 0.8203197121620178),\n",
              " ('Saxony', 0.8148468732833862)]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "wv.most_similar_cosmul(positive=['Spain','Berlin'],negative=['Madrid'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lC7lLe4KnSOf"
      },
      "source": [
        "También es posible encontrar palabras extrañas dentro de un grupo de palabras. \n",
        "\n",
        "Por ejemplo, ¿cuál de las siguientes palabras no encaja en la lista ``[Jupyter, Earth, Saturday, Mars, Moon]``?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "f0-HDwOunA-o",
        "outputId": "dc028d90-4163-4075-a5ca-4b9e0498c7e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py:895: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Saturday'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "wv.doesnt_match(['Jupyter','Earth','Saturday','Mars','Moon'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXLO9c8sn1Lr"
      },
      "source": [
        "**Ejercicio** ¿Qué palabra no encaja en la siguiente lista ``[April, May, September, Monday, July]``?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "LiU1DominA7Q",
        "outputId": "22246ff8-b6bb-451a-905f-5433b3190a55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py:895: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Monday'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "wv.doesnt_match(['April', 'May', 'September', 'Monday', 'July'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Nsf3wTtu7sX"
      },
      "source": [
        "## Glove y Fasttext\n",
        "\n",
        "Además de word2vec, han aparecido otros embeddings. Los más conocidos son [Glove](https://nlp.stanford.edu/projects/glove/) y [FastText](https://fasttext.cc/). \n",
        "\n",
        "Como hemos explicado anteriormente, el método word2vec aprende la representación de las palabras mediante una red neuronal. En cambio estos otros dos embeddings funcionan de manera un poco distinta.\n",
        "\n",
        "Glove se basa en técnicas de factorización de matrices. Para ello comienza construyendo una gran matriz con tantas filas y columnas como palabras. En esta matriz la entrada i,j indica el número de veces que la palabra i aparece en la misma frase que la palabra j. Seguidamente dicha matriz de co-ocurrencias se factoriza para producir una representación de baja dimensión.  \n",
        "\n",
        "Tanto Glove como wor2vec tienen el problema de que no sirven para codificar palabras \"raras\" o que no aparecen en el vocabulario. Para resolver dicho problema surgió FastText. \n",
        "\n",
        "FastText es una extensión del modelo word2vec. En lugar de aprender vectores para cada palabra directamente, FastText representa cada palabra como un n-grama de caracteres. Por ejemplo, si tomamos la palabra artificial y usamos n=3, la representación de dicha palabra viene dada por <ar, art, rti, tif, ifi, fic, ici, ial, al> donde < y > indican respectivamente el principio y final de una palabra. \n",
        "\n",
        "Este método ayuda a capturar el signficado de palabras más cortas y permite comprender los sufijos y prefijos. Una vez que las palabras son partidas en n-gramas se entrena un modelo similar al de word2vec. Una ventaja de FastText es que funciona con palabras raras que no habían sido vistas anteriormente (cosa que no ocurría con los otros modelos). \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSM_IwMuy46m"
      },
      "source": [
        "Desde el punto de vista de su uso no hay diferencias con respecto a word2vec. Vamos a comenzar descargando un modelo glove y otro modelo fasttext. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3d7cweK-y9JV"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader as api\n",
        "wvGlove = api.load('glove-twitter-25')\n",
        "wvFastText = api.load('fasttext-wiki-news-subwords-300')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWFsdPATzKUm"
      },
      "source": [
        "**Ejercicio** ¿Qué otros modelos de fasttext y glove proporciona la librería gensym? Consulta lo que hace la función ``api.info()``."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2k4QObeczXjj",
        "outputId": "3c506532-da76-4633-dd59-8d0defa78bd8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'corpora': {'20-newsgroups': {'checksum': 'c92fd4f6640a86d5ba89eaad818a9891',\n",
              "   'description': 'The notorious collection of approximately 20,000 newsgroup posts, partitioned (nearly) evenly across 20 different newsgroups.',\n",
              "   'fields': {'data': '',\n",
              "    'id': 'original id inferred from folder name',\n",
              "    'set': \"marker of original split (possible values 'train' and 'test')\",\n",
              "    'topic': 'name of topic (20 variant of possible values)'},\n",
              "   'file_name': '20-newsgroups.gz',\n",
              "   'file_size': 14483581,\n",
              "   'license': 'not found',\n",
              "   'num_records': 18846,\n",
              "   'parts': 1,\n",
              "   'read_more': ['http://qwone.com/~jason/20Newsgroups/'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/20-newsgroups/__init__.py',\n",
              "   'record_format': 'dict'},\n",
              "  '__testing_matrix-synopsis': {'checksum': '1767ac93a089b43899d54944b07d9dc5',\n",
              "   'description': '[THIS IS ONLY FOR TESTING] Synopsis of the movie matrix.',\n",
              "   'file_name': '__testing_matrix-synopsis.gz',\n",
              "   'parts': 1,\n",
              "   'read_more': ['http://www.imdb.com/title/tt0133093/plotsummary?ref_=ttpl_pl_syn#synopsis']},\n",
              "  '__testing_multipart-matrix-synopsis': {'checksum-0': 'c8b0c7d8cf562b1b632c262a173ac338',\n",
              "   'checksum-1': '5ff7fc6818e9a5d9bc1cf12c35ed8b96',\n",
              "   'checksum-2': '966db9d274d125beaac7987202076cba',\n",
              "   'description': '[THIS IS ONLY FOR TESTING] Synopsis of the movie matrix.',\n",
              "   'file_name': '__testing_multipart-matrix-synopsis.gz',\n",
              "   'parts': 3,\n",
              "   'read_more': ['http://www.imdb.com/title/tt0133093/plotsummary?ref_=ttpl_pl_syn#synopsis']},\n",
              "  'fake-news': {'checksum': '5e64e942df13219465927f92dcefd5fe',\n",
              "   'description': \"News dataset, contains text and metadata from 244 websites and represents 12,999 posts in total from a specific window of 30 days. The data was pulled using the webhose.io API, and because it's coming from their crawler, not all websites identified by their BS Detector are present in this dataset. Data sources that were missing a label were simply assigned a label of 'bs'. There are (ostensibly) no genuine, reliable, or trustworthy news sources represented in this dataset (so far), so don't trust anything you read.\",\n",
              "   'fields': {'author': 'author of story',\n",
              "    'comments': 'number of Facebook comments',\n",
              "    'country': 'data from webhose.io',\n",
              "    'crawled': 'date the story was archived',\n",
              "    'domain_rank': 'data from webhose.io',\n",
              "    'language': 'data from webhose.io',\n",
              "    'likes': 'number of Facebook likes',\n",
              "    'main_img_url': 'image from story',\n",
              "    'ord_in_thread': '',\n",
              "    'participants_count': 'number of participants',\n",
              "    'published': 'date published',\n",
              "    'replies_count': 'number of replies',\n",
              "    'shares': 'number of Facebook shares',\n",
              "    'site_url': 'site URL from BS detector',\n",
              "    'spam_score': 'data from webhose.io',\n",
              "    'text': 'text of story',\n",
              "    'thread_title': '',\n",
              "    'title': 'title of story',\n",
              "    'type': 'type of website (label from BS detector)',\n",
              "    'uuid': 'unique identifier'},\n",
              "   'file_name': 'fake-news.gz',\n",
              "   'file_size': 20102776,\n",
              "   'license': 'https://creativecommons.org/publicdomain/zero/1.0/',\n",
              "   'num_records': 12999,\n",
              "   'parts': 1,\n",
              "   'read_more': ['https://www.kaggle.com/mrisdal/fake-news'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/fake-news/__init__.py',\n",
              "   'record_format': 'dict'},\n",
              "  'patent-2017': {'checksum-0': '818501f0b9af62d3b88294d86d509f8f',\n",
              "   'checksum-1': '66c05635c1d3c7a19b4a335829d09ffa',\n",
              "   'description': \"Patent Grant Full Text. Contains the full text including tables, sequence data and 'in-line' mathematical expressions of each patent grant issued in 2017.\",\n",
              "   'file_name': 'patent-2017.gz',\n",
              "   'file_size': 3087262469,\n",
              "   'license': 'not found',\n",
              "   'num_records': 353197,\n",
              "   'parts': 2,\n",
              "   'read_more': ['http://patents.reedtech.com/pgrbft.php'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/patent-2017/__init__.py',\n",
              "   'record_format': 'dict'},\n",
              "  'quora-duplicate-questions': {'checksum': 'd7cfa7fbc6e2ec71ab74c495586c6365',\n",
              "   'description': 'Over 400,000 lines of potential question duplicate pairs. Each line contains IDs for each question in the pair, the full text for each question, and a binary value that indicates whether the line contains a duplicate pair or not.',\n",
              "   'fields': {'id': 'the id of a training set question pair',\n",
              "    'is_duplicate': 'the target variable, set to 1 if question1 and question2 have essentially the same meaning, and 0 otherwise',\n",
              "    'qid1': 'unique ids of each question',\n",
              "    'qid2': 'unique ids of each question',\n",
              "    'question1': 'the full text of each question',\n",
              "    'question2': 'the full text of each question'},\n",
              "   'file_name': 'quora-duplicate-questions.gz',\n",
              "   'file_size': 21684784,\n",
              "   'license': 'probably https://www.quora.com/about/tos',\n",
              "   'num_records': 404290,\n",
              "   'parts': 1,\n",
              "   'read_more': ['https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/quora-duplicate-questions/__init__.py',\n",
              "   'record_format': 'dict'},\n",
              "  'semeval-2016-2017-task3-subtaskA-unannotated': {'checksum': '2de0e2f2c4f91c66ae4fcf58d50ba816',\n",
              "   'description': 'SemEval 2016 / 2017 Task 3 Subtask A unannotated dataset contains 189,941 questions and 1,894,456 comments in English collected from the Community Question Answering (CQA) web forum of Qatar Living. These can be used as a corpus for language modelling.',\n",
              "   'fields': {'RelComments': [{'RELC_DATE': 'date of posting',\n",
              "      'RELC_ID': 'comment identifier',\n",
              "      'RELC_USERID': 'identifier of the user posting the comment',\n",
              "      'RELC_USERNAME': 'name of the user posting the comment',\n",
              "      'RelCText': 'text of answer'}],\n",
              "    'RelQuestion': {'RELQ_CATEGORY': 'question category, according to the Qatar Living taxonomy',\n",
              "     'RELQ_DATE': 'date of posting',\n",
              "     'RELQ_ID': 'question indentifier',\n",
              "     'RELQ_USERID': 'identifier of the user asking the question',\n",
              "     'RELQ_USERNAME': 'name of the user asking the question',\n",
              "     'RelQBody': 'body of question',\n",
              "     'RelQSubject': 'subject of question'},\n",
              "    'THREAD_SEQUENCE': ''},\n",
              "   'file_name': 'semeval-2016-2017-task3-subtaskA-unannotated.gz',\n",
              "   'file_size': 234373151,\n",
              "   'license': 'These datasets are free for general research use.',\n",
              "   'num_records': 189941,\n",
              "   'parts': 1,\n",
              "   'read_more': ['http://alt.qcri.org/semeval2016/task3/',\n",
              "    'http://alt.qcri.org/semeval2016/task3/data/uploads/semeval2016-task3-report.pdf',\n",
              "    'https://github.com/RaRe-Technologies/gensim-data/issues/18',\n",
              "    'https://github.com/Witiko/semeval-2016_2017-task3-subtaskA-unannotated-english'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/semeval-2016-2017-task3-subtaskA-unannotated-eng/__init__.py',\n",
              "   'record_format': 'dict'},\n",
              "  'semeval-2016-2017-task3-subtaskBC': {'checksum': '701ea67acd82e75f95e1d8e62fb0ad29',\n",
              "   'description': 'SemEval 2016 / 2017 Task 3 Subtask B and C datasets contain train+development (317 original questions, 3,169 related questions, and 31,690 comments), and test datasets in English. The description of the tasks and the collected data is given in sections 3 and 4.1 of the task paper http://alt.qcri.org/semeval2016/task3/data/uploads/semeval2016-task3-report.pdf linked in section “Papers” of https://github.com/RaRe-Technologies/gensim-data/issues/18.',\n",
              "   'fields': {'2016-dev': ['...'],\n",
              "    '2016-test': ['...'],\n",
              "    '2016-train': ['...'],\n",
              "    '2017-test': ['...']},\n",
              "   'file_name': 'semeval-2016-2017-task3-subtaskBC.gz',\n",
              "   'file_size': 6344358,\n",
              "   'license': 'All files released for the task are free for general research use',\n",
              "   'num_records': -1,\n",
              "   'parts': 1,\n",
              "   'read_more': ['http://alt.qcri.org/semeval2017/task3/',\n",
              "    'http://alt.qcri.org/semeval2017/task3/data/uploads/semeval2017-task3.pdf',\n",
              "    'https://github.com/RaRe-Technologies/gensim-data/issues/18',\n",
              "    'https://github.com/Witiko/semeval-2016_2017-task3-subtaskB-english'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/semeval-2016-2017-task3-subtaskB-eng/__init__.py',\n",
              "   'record_format': 'dict'},\n",
              "  'text8': {'checksum': '68799af40b6bda07dfa47a32612e5364',\n",
              "   'description': 'First 100,000,000 bytes of plain text from Wikipedia. Used for testing purposes; see wiki-english-* for proper full Wikipedia datasets.',\n",
              "   'file_name': 'text8.gz',\n",
              "   'file_size': 33182058,\n",
              "   'license': 'not found',\n",
              "   'num_records': 1701,\n",
              "   'parts': 1,\n",
              "   'read_more': ['http://mattmahoney.net/dc/textdata.html'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/text8/__init__.py',\n",
              "   'record_format': 'list of str (tokens)'},\n",
              "  'wiki-english-20171001': {'checksum-0': 'a7d7d7fd41ea7e2d7fa32ec1bb640d71',\n",
              "   'checksum-1': 'b2683e3356ffbca3b6c2dca6e9801f9f',\n",
              "   'checksum-2': 'c5cde2a9ae77b3c4ebce804f6df542c2',\n",
              "   'checksum-3': '00b71144ed5e3aeeb885de84f7452b81',\n",
              "   'description': 'Extracted Wikipedia dump from October 2017. Produced by `python -m gensim.scripts.segment_wiki -f enwiki-20171001-pages-articles.xml.bz2 -o wiki-en.gz`',\n",
              "   'fields': {'section_texts': 'list of body of sections',\n",
              "    'section_titles': 'list of titles of sections',\n",
              "    'title': 'Title of wiki article'},\n",
              "   'file_name': 'wiki-english-20171001.gz',\n",
              "   'file_size': 6516051717,\n",
              "   'license': 'https://dumps.wikimedia.org/legal.html',\n",
              "   'num_records': 4924894,\n",
              "   'parts': 4,\n",
              "   'read_more': ['https://dumps.wikimedia.org/enwiki/20171001/'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/wiki-english-20171001/__init__.py',\n",
              "   'record_format': 'dict'}},\n",
              " 'models': {'__testing_word2vec-matrix-synopsis': {'checksum': '534dcb8b56a360977a269b7bfc62d124',\n",
              "   'description': '[THIS IS ONLY FOR TESTING] Word vecrors of the movie matrix.',\n",
              "   'file_name': '__testing_word2vec-matrix-synopsis.gz',\n",
              "   'parameters': {'dimensions': 50},\n",
              "   'parts': 1,\n",
              "   'preprocessing': 'Converted to w2v using a preprocessed corpus. Converted to w2v format with `python3.5 -m gensim.models.word2vec -train <input_filename> -iter 50 -output <output_filename>`.',\n",
              "   'read_more': []},\n",
              "  'conceptnet-numberbatch-17-06-300': {'base_dataset': 'ConceptNet, word2vec, GloVe, and OpenSubtitles 2016',\n",
              "   'checksum': 'fd642d457adcd0ea94da0cd21b150847',\n",
              "   'description': 'ConceptNet Numberbatch consists of state-of-the-art semantic vectors (also known as word embeddings) that can be used directly as a representation of word meanings or as a starting point for further machine learning. ConceptNet Numberbatch is part of the ConceptNet open data project. ConceptNet provides lots of ways to compute with word meanings, one of which is word embeddings. ConceptNet Numberbatch is a snapshot of just the word embeddings. It is built using an ensemble that combines data from ConceptNet, word2vec, GloVe, and OpenSubtitles 2016, using a variation on retrofitting.',\n",
              "   'file_name': 'conceptnet-numberbatch-17-06-300.gz',\n",
              "   'file_size': 1225497562,\n",
              "   'license': 'https://github.com/commonsense/conceptnet-numberbatch/blob/master/LICENSE.txt',\n",
              "   'num_records': 1917247,\n",
              "   'parameters': {'dimension': 300},\n",
              "   'parts': 1,\n",
              "   'read_more': ['http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14972',\n",
              "    'https://github.com/commonsense/conceptnet-numberbatch',\n",
              "    'http://conceptnet.io/'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/conceptnet-numberbatch-17-06-300/__init__.py'},\n",
              "  'fasttext-wiki-news-subwords-300': {'base_dataset': 'Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset (16B tokens)',\n",
              "   'checksum': 'de2bb3a20c46ce65c9c131e1ad9a77af',\n",
              "   'description': '1 million word vectors trained on Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset (16B tokens).',\n",
              "   'file_name': 'fasttext-wiki-news-subwords-300.gz',\n",
              "   'file_size': 1005007116,\n",
              "   'license': 'https://creativecommons.org/licenses/by-sa/3.0/',\n",
              "   'num_records': 999999,\n",
              "   'parameters': {'dimension': 300},\n",
              "   'parts': 1,\n",
              "   'read_more': ['https://fasttext.cc/docs/en/english-vectors.html',\n",
              "    'https://arxiv.org/abs/1712.09405',\n",
              "    'https://arxiv.org/abs/1607.01759'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/fasttext-wiki-news-subwords-300/__init__.py'},\n",
              "  'glove-twitter-100': {'base_dataset': 'Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)',\n",
              "   'checksum': 'b04f7bed38756d64cf55b58ce7e97b15',\n",
              "   'description': 'Pre-trained vectors based on  2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/)',\n",
              "   'file_name': 'glove-twitter-100.gz',\n",
              "   'file_size': 405932991,\n",
              "   'license': 'http://opendatacommons.org/licenses/pddl/',\n",
              "   'num_records': 1193514,\n",
              "   'parameters': {'dimension': 100},\n",
              "   'parts': 1,\n",
              "   'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-100.txt`.',\n",
              "   'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
              "    'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-100/__init__.py'},\n",
              "  'glove-twitter-200': {'base_dataset': 'Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)',\n",
              "   'checksum': 'e52e8392d1860b95d5308a525817d8f9',\n",
              "   'description': 'Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/).',\n",
              "   'file_name': 'glove-twitter-200.gz',\n",
              "   'file_size': 795373100,\n",
              "   'license': 'http://opendatacommons.org/licenses/pddl/',\n",
              "   'num_records': 1193514,\n",
              "   'parameters': {'dimension': 200},\n",
              "   'parts': 1,\n",
              "   'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-200.txt`.',\n",
              "   'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
              "    'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-200/__init__.py'},\n",
              "  'glove-twitter-25': {'base_dataset': 'Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)',\n",
              "   'checksum': '50db0211d7e7a2dcd362c6b774762793',\n",
              "   'description': 'Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/).',\n",
              "   'file_name': 'glove-twitter-25.gz',\n",
              "   'file_size': 109885004,\n",
              "   'license': 'http://opendatacommons.org/licenses/pddl/',\n",
              "   'num_records': 1193514,\n",
              "   'parameters': {'dimension': 25},\n",
              "   'parts': 1,\n",
              "   'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-25.txt`.',\n",
              "   'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
              "    'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-25/__init__.py'},\n",
              "  'glove-twitter-50': {'base_dataset': 'Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)',\n",
              "   'checksum': 'c168f18641f8c8a00fe30984c4799b2b',\n",
              "   'description': 'Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/)',\n",
              "   'file_name': 'glove-twitter-50.gz',\n",
              "   'file_size': 209216938,\n",
              "   'license': 'http://opendatacommons.org/licenses/pddl/',\n",
              "   'num_records': 1193514,\n",
              "   'parameters': {'dimension': 50},\n",
              "   'parts': 1,\n",
              "   'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-50.txt`.',\n",
              "   'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
              "    'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-50/__init__.py'},\n",
              "  'glove-wiki-gigaword-100': {'base_dataset': 'Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)',\n",
              "   'checksum': '40ec481866001177b8cd4cb0df92924f',\n",
              "   'description': 'Pre-trained vectors based on Wikipedia 2014 + Gigaword 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).',\n",
              "   'file_name': 'glove-wiki-gigaword-100.gz',\n",
              "   'file_size': 134300434,\n",
              "   'license': 'http://opendatacommons.org/licenses/pddl/',\n",
              "   'num_records': 400000,\n",
              "   'parameters': {'dimension': 100},\n",
              "   'parts': 1,\n",
              "   'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-100.txt`.',\n",
              "   'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
              "    'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-100/__init__.py'},\n",
              "  'glove-wiki-gigaword-200': {'base_dataset': 'Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)',\n",
              "   'checksum': '59652db361b7a87ee73834a6c391dfc1',\n",
              "   'description': 'Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).',\n",
              "   'file_name': 'glove-wiki-gigaword-200.gz',\n",
              "   'file_size': 264336934,\n",
              "   'license': 'http://opendatacommons.org/licenses/pddl/',\n",
              "   'num_records': 400000,\n",
              "   'parameters': {'dimension': 200},\n",
              "   'parts': 1,\n",
              "   'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-200.txt`.',\n",
              "   'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
              "    'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-200/__init__.py'},\n",
              "  'glove-wiki-gigaword-300': {'base_dataset': 'Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)',\n",
              "   'checksum': '29e9329ac2241937d55b852e8284e89b',\n",
              "   'description': 'Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).',\n",
              "   'file_name': 'glove-wiki-gigaword-300.gz',\n",
              "   'file_size': 394362229,\n",
              "   'license': 'http://opendatacommons.org/licenses/pddl/',\n",
              "   'num_records': 400000,\n",
              "   'parameters': {'dimension': 300},\n",
              "   'parts': 1,\n",
              "   'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-300.txt`.',\n",
              "   'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
              "    'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-300/__init__.py'},\n",
              "  'glove-wiki-gigaword-50': {'base_dataset': 'Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)',\n",
              "   'checksum': 'c289bc5d7f2f02c6dc9f2f9b67641813',\n",
              "   'description': 'Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).',\n",
              "   'file_name': 'glove-wiki-gigaword-50.gz',\n",
              "   'file_size': 69182535,\n",
              "   'license': 'http://opendatacommons.org/licenses/pddl/',\n",
              "   'num_records': 400000,\n",
              "   'parameters': {'dimension': 50},\n",
              "   'parts': 1,\n",
              "   'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-50.txt`.',\n",
              "   'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
              "    'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-50/__init__.py'},\n",
              "  'word2vec-google-news-300': {'base_dataset': 'Google News (about 100 billion words)',\n",
              "   'checksum': 'a5e5354d40acb95f9ec66d5977d140ef',\n",
              "   'description': \"Pre-trained vectors trained on a part of the Google News dataset (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases. The phrases were obtained using a simple data-driven approach described in 'Distributed Representations of Words and Phrases and their Compositionality' (https://code.google.com/archive/p/word2vec/).\",\n",
              "   'file_name': 'word2vec-google-news-300.gz',\n",
              "   'file_size': 1743563840,\n",
              "   'license': 'not found',\n",
              "   'num_records': 3000000,\n",
              "   'parameters': {'dimension': 300},\n",
              "   'parts': 1,\n",
              "   'read_more': ['https://code.google.com/archive/p/word2vec/',\n",
              "    'https://arxiv.org/abs/1301.3781',\n",
              "    'https://arxiv.org/abs/1310.4546',\n",
              "    'https://www.microsoft.com/en-us/research/publication/linguistic-regularities-in-continuous-space-word-representations/?from=http%3A%2F%2Fresearch.microsoft.com%2Fpubs%2F189726%2Frvecs.pdf'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/word2vec-google-news-300/__init__.py'},\n",
              "  'word2vec-ruscorpora-300': {'base_dataset': 'Russian National Corpus (about 250M words)',\n",
              "   'checksum': '9bdebdc8ae6d17d20839dd9b5af10bc4',\n",
              "   'description': 'Word2vec Continuous Skipgram vectors trained on full Russian National Corpus (about 250M words). The model contains 185K words.',\n",
              "   'file_name': 'word2vec-ruscorpora-300.gz',\n",
              "   'file_size': 208427381,\n",
              "   'license': 'https://creativecommons.org/licenses/by/4.0/deed.en',\n",
              "   'num_records': 184973,\n",
              "   'parameters': {'dimension': 300, 'window_size': 10},\n",
              "   'parts': 1,\n",
              "   'preprocessing': 'The corpus was lemmatized and tagged with Universal PoS',\n",
              "   'read_more': ['https://www.academia.edu/24306935/WebVectors_a_Toolkit_for_Building_Web_Interfaces_for_Vector_Semantic_Models',\n",
              "    'http://rusvectores.org/en/',\n",
              "    'https://github.com/RaRe-Technologies/gensim-data/issues/3'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/word2vec-ruscorpora-300/__init__.py'}}}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "api.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7rju0YKzj9Z"
      },
      "source": [
        "**Ejercicio** Compara los resultados proporcionados por cada uno de los embeddings para los ejercicios presentados en el apartado anterior. Añade tantas celdas como necesites. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Iré alternando los embeddings para que se pueda ver los resultados de una forma más visual"
      ],
      "metadata": {
        "id": "XlJ-1QBtU_ih"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pa8sVHesz_tt"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    vec_cameroon = wvGlove['cameroon']\n",
        "except KeyError:\n",
        "    print(\"The word 'cameroon' does not appear in this model\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    vec_cameroon = wvFastText['cameroon']\n",
        "except KeyError:\n",
        "    print(\"The word 'cameroon' does not appear in this model\")"
      ],
      "metadata": {
        "id": "0fWLobX6VH49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "VT9vWaf1VKfd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da5c991c-d5b1-4d59-a906-9dbe87de6108",
        "id": "AQzU6EQ4U2Ol"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('front', 0.936506986618042), ('on', 0.9070020318031311), ('table', 0.8939012885093689), ('truck', 0.8898833394050598), ('place', 0.8800071477890015)]\n"
          ]
        }
      ],
      "source": [
        "print(wvGlove.most_similar(positive=['car'], topn=5))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(wvFastText.most_similar(positive=['car'], topn=5)) # al ejecutar esto sube exponencialmente el uso de la CPU"
      ],
      "metadata": {
        "id": "2FVx8dQrVPPU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0945710-582a-4322-ead2-d006a8f6f37d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('cars', 0.7954095005989075), ('vehicle', 0.7870852947235107), ('non-car', 0.7824047803878784), ('automobile', 0.7790266275405884), ('super-car', 0.7759445905685425)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "w4JpqY4NVhLm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxgmEaNvU2Op",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac8d6c03-2a3f-4686-9a36-3939579e7844"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('meets', 1.0724927186965942),\n",
              " ('crow', 1.03579580783844),\n",
              " ('hedgehog', 1.0280965566635132),\n",
              " ('prince', 1.024889349937439),\n",
              " ('hunter', 1.0226764678955078),\n",
              " ('mercy', 1.0204172134399414),\n",
              " ('queen', 1.0198343992233276),\n",
              " ('shepherd', 1.0195919275283813),\n",
              " ('soldier', 1.019392728805542),\n",
              " ('widow', 1.0162571668624878)]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "wvGlove.most_similar_cosmul(positive=['king','woman'],negative=['man'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wvFastText.most_similar_cosmul(positive=['king','woman'],negative=['man'])"
      ],
      "metadata": {
        "id": "hrdLowoQVo51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1677cc8d-fa20-4d4d-f70c-4d3e1246aa47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('queen', 0.9390855431556702),\n",
              " ('queen-mother', 0.9078598618507385),\n",
              " ('king-', 0.8828967213630676),\n",
              " ('queen-consort', 0.882541835308075),\n",
              " ('child-king', 0.8680858016014099),\n",
              " ('monarch', 0.8670082688331604),\n",
              " ('ex-queen', 0.8654636740684509),\n",
              " ('princess', 0.8628991842269897),\n",
              " ('queen-', 0.8613532781600952),\n",
              " ('boy-king', 0.8604660630226135)]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ofviXGlOVqLO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DuSgLdC1U2Oq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd488f5b-59c5-41e5-85bc-6209b923fa6d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('gabi', 1.0315500497817993),\n",
              " ('isa', 1.0036530494689941),\n",
              " ('na', 0.9959880113601685),\n",
              " ('k', 0.995937168598175),\n",
              " ('po', 0.9776562452316284),\n",
              " ('agt', 0.9753057360649109),\n",
              " ('ba', 0.974004864692688),\n",
              " ('tou', 0.9657711982727051),\n",
              " ('dae', 0.9655082821846008),\n",
              " ('julia', 0.9648900628089905)]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "wvGlove.most_similar_cosmul(positive=['ate','go'],negative=['eat'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wvFastText.most_similar_cosmul(positive=['ate','go'],negative=['eat'])"
      ],
      "metadata": {
        "id": "50wxHvcvVrpF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39f8a254-5926-4cf9-b0c1-0a1dcf5a91ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('went', 0.9635580778121948),\n",
              " ('gone', 0.8864149451255798),\n",
              " ('came', 0.8734760880470276),\n",
              " ('stayed', 0.8679504990577698),\n",
              " ('goes', 0.8597913980484009),\n",
              " ('followed', 0.8560923337936401),\n",
              " ('looked', 0.854744553565979),\n",
              " ('walked', 0.8542968034744263),\n",
              " ('got', 0.8524806499481201),\n",
              " ('wended', 0.852008581161499)]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "RSExxDjIVuuO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3vgzTMvU2Oq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13fa308a-3abe-4e8a-c54e-07449cb84ab5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('loоk', 1.1995182037353516),\n",
              " ('lооk', 1.165311336517334),\n",
              " ('латинский', 1.139757752418518),\n",
              " ('brussels', 1.1365525722503662),\n",
              " ('lоok', 1.1363283395767212),\n",
              " ('толковый', 1.120051383972168),\n",
              " ('герл', 1.115322470664978),\n",
              " ('트친분들에게', 1.114218831062317),\n",
              " ('cornwall', 1.1134241819381714),\n",
              " ('аt', 1.1115261316299438)]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "wvGlove.most_similar_cosmul(positive=['spain','berlin'],negative=['madrid'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wvFastText.most_similar_cosmul(positive=['Spain','Berlin'],negative=['Madrid'])"
      ],
      "metadata": {
        "id": "VcrA6pL-VxYx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebcda832-ea22-4cf3-9a63-0b3da02a535e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Germany', 0.932011604309082),\n",
              " ('Germanies', 0.8789596557617188),\n",
              " ('Prussia', 0.8766506314277649),\n",
              " ('Poland', 0.8737272620201111),\n",
              " ('Austria', 0.8685898780822754),\n",
              " ('Britain', 0.8673689961433411),\n",
              " ('Europe', 0.8630521297454834),\n",
              " ('West-Berlin', 0.862476110458374),\n",
              " ('Denmark', 0.8581658601760864),\n",
              " ('France', 0.8580948710441589)]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "m46YloDzVz3m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9WWj1x3U2Or",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "7f24876a-fc24-4052-f4ed-974d813bf393"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py:895: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'saturday'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "wvGlove.doesnt_match(['jupyter','earth','saturday','mars','moon'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wvFastText.doesnt_match(['Jupyter','Earth','Saturday','Mars','Moon'])"
      ],
      "metadata": {
        "id": "FCfZxf1KV17R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "8cd92440-f24d-437b-b199-a4502ab7f3f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py:895: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Saturday'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ZTiStIXNV2rX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V85Aas8hU2Os",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "74b385b9-356a-4880-c991-12912ad88fd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py:895: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'may'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "wvGlove.doesnt_match(['april', 'may', 'september', 'monday', 'july'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wvFastText.doesnt_match(['April', 'May', 'September', 'Monday', 'July'])"
      ],
      "metadata": {
        "id": "Jeb5rrpMV5h-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "aa2a924e-325f-40fd-8957-b9ca9198fb29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py:895: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Monday'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "STobu7H2V53O"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCSfcS6z0CF9"
      },
      "source": [
        "## Otros idiomas\n",
        "\n",
        "Los embeddings son particulares de cada idioma por lo que la representación de palabras en inglés no nos sirve cuando estamos trabajando en español. Afortunadamente, existen [modelos preentrenados para nuestro idioma](https://github.com/dccuchile/spanish-word-embeddings).\n",
        "\n",
        "Su uso es relativamente sencillo. Debemos descargar el fichero de los pesos y cargarlo. Seguidamente podremos usarlo como hemos visto anteriormente. Por ejemplo vamos a cargar un modelo FastText. Comenzamos descargando el modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "eQ6DvTmT0jkw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b338940-3b1c-4d23-975d-f6d51065e86c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-25 13:51:22--  https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.es.vec\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.74.142, 104.22.75.142, 172.67.9.4, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.74.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2594302560 (2.4G) [binary/octet-stream]\n",
            "Saving to: ‘wiki.es.vec’\n",
            "\n",
            "wiki.es.vec         100%[===================>]   2.42G  40.0MB/s    in 62s     \n",
            "\n",
            "2022-05-25 13:52:25 (40.0 MB/s) - ‘wiki.es.vec’ saved [2594302560/2594302560]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.es.vec -O wiki.es.vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmYlHWhO1CVo"
      },
      "source": [
        "A continuación mostramos las primeras filas del modelo, como se puede ver el documento que acabamos de descargar contiene en cada línea un n-grama (recordar qué es lo que hace FastText) y a continuación su representación en forma de vector. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "EUbCHoN-1OUN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8c63772-ff1d-4c52-a053-e24f2217d8d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "985667 300\n",
            "de -0.13075 -0.087659 -0.11427 -0.020641 0.11753 0.19687 0.054257 -0.0028717 0.062278 -0.10023 -0.050123 -0.026275 -0.057605 -0.13072 0.10147 0.15849 0.095493 0.051555 0.015874 -0.046374 0.098467 0.034867 0.039933 -0.1208 0.065478 -0.0098815 -0.13914 -0.043732 -0.015622 0.05665 -0.01476 -0.0054753 -0.047127 -0.21595 -0.015154 -0.0034798 0.058253 0.036444 -0.25157 0.060459 0.23842 0.017983 0.10673 -0.15889 0.23043 -0.078636 0.075394 -0.18431 -0.31417 0.084773 -0.14912 0.036904 -0.1144 0.025056 0.058607 0.059822 -0.17929 0.028468 0.16728 -0.020946 0.019714 0.0083937 0.032227 0.013204 0.06393 -0.19616 -0.043487 0.10124 -0.032762 0.17206 -0.062339 -0.10172 -0.31708 0.079012 -0.1232 -0.15504 -0.084187 -0.099777 0.16626 0.086791 0.001035 0.10478 0.12913 -0.0026416 0.061668 0.10004 -0.073838 0.167 0.10342 -0.05263 0.20125 0.23046 0.043589 0.19497 -0.0093385 -0.042631 -0.17599 -0.15208 0.23261 -0.10049 0.096678 -0.030501 0.060627 -0.27119 -0.11177 0.26739 0.205 -0.13012 0.051966 0.18115 0.04977 0.069596 -0.039885 0.20009 -0.13619 0.28566 0.16922 -0.084435 -0.25588 -0.019324 0.024354 0.017741 0.084594 -0.15838 -0.18982 -0.14319 0.017073 0.03242 0.21051 0.14096 0.076237 0.033525 0.0043038 0.064959 -0.037789 -0.14099 0.0075984 -0.11115 -0.035141 0.14261 0.053846 -0.057024 -0.0094945 0.10681 0.02614 0.042217 -0.070822 -0.10298 0.094206 0.045829 0.2296 -0.10944 0.10839 -0.1162 -0.062974 -0.037453 -0.041537 -0.071237 -0.086044 0.061869 -0.081216 0.13235 0.013349 0.041143 0.096013 -0.14398 0.050647 -0.21338 -0.05996 -0.076753 0.1126 0.1712 0.11117 0.0094953 -0.14779 -0.22474 -0.063649 -0.23653 -0.014093 0.13146 -0.014384 -0.10514 0.012632 -0.014102 0.0011301 0.0052141 -0.09386 0.026303 -0.19247 0.15463 -0.070266 -0.08647 0.032046 -0.141 -0.11656 -0.11 0.13072 0.075912 -0.050148 0.15808 -0.011774 0.064582 -0.17485 -0.055014 -0.21166 0.23468 0.27994 -0.0047165 -0.058851 -0.011637 0.048679 -0.065226 0.035772 -0.14549 -0.0047683 -0.2176 0.097211 -0.23592 0.059933 -0.043059 -0.142 -0.13229 0.14831 -0.10836 -0.041123 -0.053446 -0.20878 -0.056799 0.042364 -0.073509 -0.021485 0.023052 0.026784 0.01281 0.0029815 0.074967 0.20921 0.073388 0.034119 -0.14643 -0.069501 0.14438 0.0901 -0.030421 0.094469 0.03088 0.014779 -0.068274 0.1562 -0.14401 -0.070045 -0.026215 -0.028127 -0.044533 -0.11838 0.12447 -0.21847 0.027829 -0.034787 -0.1793 0.095557 -0.0069741 0.063249 -0.13063 0.02909 -0.0040399 -0.10868 0.19582 -0.23576 -0.23128 0.099063 0.026377 0.096095 0.30016 0.097604 0.019053 0.081196 0.062768 0.23626 -0.042725 -0.067281 0.1964 -0.16594 -0.17855 0.26355 -0.029399 -0.11128 0.10487 -0.1206 0.01336 -0.18132 0.0022502 0.19542 -0.18097 -0.30351 -0.10376 0.030874 -0.040476 -0.012293 0.042569 \n",
            "</s> -0.36446 0.095962 -0.16188 -0.28784 0.24135 -0.091308 -0.03191 0.087084 0.026673 -0.080837 0.15115 0.082673 0.090765 -0.092976 0.15758 0.080713 -0.092193 0.1489 -0.052053 0.12568 0.26058 -0.13323 0.25828 -0.11334 0.2739 0.083208 0.013881 -0.098315 -0.2813 -0.22978 0.2877 0.29269 0.047912 0.10456 0.011869 0.010585 0.10773 -0.11436 0.19706 0.33666 0.34352 0.055834 -0.11962 0.075398 0.19805 -0.35103 -0.045687 -0.26519 0.071122 0.080239 -0.23263 -0.039208 -0.53293 -0.034166 0.12133 -0.075486 0.048819 0.19546 -0.039053 0.0054963 0.044245 -0.23716 0.14347 -0.17614 0.40267 0.24665 0.26393 -0.010655 -0.41789 -0.0043102 -0.14959 -0.14506 -0.3274 0.057849 -0.35901 0.18165 0.22303 0.018395 0.39798 0.19296 0.0041728 -0.35316 0.13027 -0.246 -0.071848 -0.12122 0.17941 0.36235 -0.27719 -0.11889 0.57624 -0.026302 0.13594 -0.16757 -0.02421 -0.14689 -0.38124 0.07643 0.003238 0.03209 0.38536 -0.23563 0.23756 -0.5635 -0.30937 0.053787 -0.25446 0.066883 -0.13233 0.029385 -0.18011 0.23471 0.077603 0.13689 0.11513 -0.066863 0.31383 -0.25686 -0.011146 -0.18943 -0.071543 -0.083939 -0.15583 -0.24908 0.066351 -0.14104 0.20348 0.047783 0.015516 -0.13304 -0.29391 0.16016 -0.23626 -0.11463 -0.22063 0.14786 0.1471 -0.072779 -0.066447 -0.07895 0.10502 0.15527 -0.2247 0.20327 0.21791 -0.20792 0.59138 0.13196 -0.13033 -0.16515 -0.036654 -0.22644 -0.0061621 0.039057 0.10497 -0.3722 -0.30235 -0.29848 0.10427 -0.28852 0.016451 0.020937 -0.089796 0.043252 -0.48395 0.11963 0.42526 -0.0025859 -0.1585 -0.35239 -0.094638 0.085925 0.13943 -0.012928 0.17401 -0.085624 0.24526 -0.33238 -0.16465 -0.23434 -0.22478 0.093202 0.18044 -0.0022709 0.64433 0.17787 -0.16303 0.020501 0.29836 0.18501 -0.31519 -0.062988 -0.11118 -0.27248 0.0066759 0.15482 -0.034834 -0.13778 0.17437 -0.1289 0.26582 0.18493 0.137 0.089572 -0.34967 0.070412 -0.10905 -0.13934 -0.071264 -0.21024 -0.23149 -0.15622 0.031041 -0.25423 0.029135 -0.13781 0.33374 -0.010971 0.025904 0.0727 -0.20574 -0.018906 0.17188 -0.28261 -0.24606 -0.18171 0.045923 -0.004392 0.15882 0.10441 -0.21117 -0.021247 -0.070883 -0.20569 0.056579 0.013914 0.017319 0.031098 0.19314 0.082517 -0.31496 0.15507 0.31209 0.49492 0.092639 -0.12091 -0.15506 -0.038429 0.05822 -0.59395 0.028685 0.33887 0.11506 -0.070859 -0.094241 0.2186 -0.065878 0.22759 -0.43652 -0.082678 -0.023709 -0.10877 -0.10187 0.23392 0.21493 -0.0013937 -0.11347 0.084367 0.08499 0.15615 0.16991 0.21016 0.045504 0.014265 0.40636 -0.033568 -0.19224 0.1565 0.16724 0.16809 -0.058226 0.30785 -0.25357 -0.41269 0.20236 -0.065041 0.088534 0.16504 -0.092149 0.063091 -0.19525 0.016894 0.28062 0.089901 -0.32328 -0.31542 -0.35193 -0.14986 0.23584 0.18541 \n",
            ", -0.05911 -0.083343 -0.093019 0.055022 0.11158 0.26959 0.063027 0.029276 0.047163 -0.21404 0.076856 -0.0048167 0.073276 -0.071174 0.067919 0.16795 0.025599 0.046711 0.0083535 0.074723 0.15508 0.12074 -0.15292 -0.22711 0.0059846 -0.032974 -0.17791 -0.13321 0.080922 -0.0015115 0.081788 0.070111 -0.0059703 -0.24145 0.15036 0.10163 -0.19595 0.20332 -0.08821 -0.0025969 0.12682 -0.026486 0.05218 -0.016389 0.18476 -0.052947 0.11276 -0.13077 -0.049243 0.0099811 -0.039274 -0.070482 -0.19867 -0.14601 -0.089112 -0.00024724 -0.051753 0.018324 0.17996 -0.076272 -0.12699 0.051751 0.099054 0.077612 -0.070409 -0.10894 -0.06308 0.1683 0.011339 0.24431 0.0039653 -0.028868 -0.10322 0.044214 -0.036952 0.0037858 -0.11704 -0.10345 0.20865 0.16026 0.0032515 0.0085871 0.15769 0.073399 0.03924 0.10125 0.0037511 0.32106 0.16057 -0.054304 0.22096 0.14089 0.12211 -0.015545 -0.065033 -0.037958 -0.23508 0.11448 -0.031709 0.083186 0.14368 -0.021441 0.32513 -0.090612 -0.088993 0.24537 0.24262 0.078445 0.05634 -0.0067866 0.045198 0.063821 0.042502 -0.026299 -0.21504 0.14574 0.16102 -0.056195 -0.059683 0.029853 -0.061782 -0.012889 0.082538 -0.097795 -0.34828 -0.11273 0.01714 -0.0049159 0.12113 0.03202 0.023385 -0.0081865 0.17709 0.13416 -0.15158 0.033909 -0.0078484 -0.08605 0.17532 0.10965 -0.082889 0.085272 0.044922 0.079424 -0.20138 0.052378 -0.0020818 -0.15343 0.11117 0.14514 0.091916 0.0010245 0.1316 -0.033164 0.047423 0.0066566 -0.02126 -0.041627 0.093807 0.051691 -0.031415 0.09157 -0.14212 0.12573 -0.021071 -0.12709 0.046962 -0.12354 -0.23961 -0.07021 0.1694 0.14152 -0.041632 0.034251 -0.10315 -0.028185 -0.15899 0.051614 0.096846 0.061382 0.10459 -0.0062639 -0.082858 -0.1484 0.16915 0.061101 0.017711 0.061895 -0.15745 0.1938 -0.099898 -0.074984 0.16214 -0.16765 -0.1896 0.0028484 -0.091578 -0.067959 -0.049052 0.052995 0.13692 0.077207 0.060084 0.055329 -0.015046 0.14196 0.098207 0.04155 -0.046994 -0.23547 -0.045419 -0.099123 -0.042548 -0.21635 0.073838 -0.015737 0.0029493 -0.092031 0.058939 0.019329 0.083679 -0.15091 0.055219 -0.022681 -0.10084 -0.0061851 -0.10521 0.035751 0.1228 -0.085189 0.03289 -0.04721 0.15267 0.14318 0.020812 0.036774 0.13577 -0.094081 -0.026113 -0.1048 -0.048832 0.08081 0.054203 -0.030686 0.0038586 -0.066868 0.033404 0.054518 -0.018115 -0.044879 0.041789 0.01849 0.051579 -0.0049403 -0.092841 0.090414 -0.077543 0.059121 0.083407 -0.020383 -0.13688 0.11172 -0.02245 0.052503 0.055886 -0.059816 -0.031772 0.13666 -0.18941 -0.19601 -0.015958 0.05368 0.052969 0.16351 0.1032 -0.01745 0.17995 -0.0055358 0.2025 -0.17807 -0.028202 -0.14807 -0.07034 -0.14238 0.02811 0.026752 0.12816 0.1624 -0.11337 0.16677 -0.16916 -0.18453 0.15117 -0.030142 -0.28381 -0.23216 -0.1324 -0.054064 0.17285 0.16713 \n",
            ". 0.0029007 0.09963 -0.055262 -0.087999 -0.081318 0.23905 0.11621 0.043504 0.024316 -0.1463 0.13194 0.11518 0.073063 -0.19451 0.25814 0.18096 0.13045 -0.026367 -0.079153 -0.058566 0.25584 0.12697 -0.15602 -0.26563 -0.043 0.11169 -0.16126 -0.18886 0.10825 0.0275 -0.050402 0.00065111 0.050345 -0.35321 0.0066056 0.10832 -0.29408 0.066805 -0.069649 0.023978 0.10978 -0.091219 0.033005 -0.087315 0.18795 -0.078058 0.054102 -0.21036 -0.082037 0.00074696 -0.14648 -0.029418 -0.40931 -0.096455 0.015774 0.069467 0.056977 0.021412 0.069897 0.0091133 0.066669 0.012677 0.02872 -0.00033325 0.11125 -0.04982 -0.25344 0.13265 0.0050706 0.026485 0.0061715 0.029646 -0.15103 0.040158 0.042827 -0.068451 -0.032853 -0.23429 0.14526 0.0050865 0.080915 -0.1392 0.21399 0.01289 0.036947 0.26361 0.049181 0.22027 0.18195 -0.11468 0.27258 0.13962 0.15473 0.091993 -0.25171 -0.1323 -0.34287 0.001294 -0.091296 -0.17397 0.27088 0.046106 0.085174 -0.22841 -0.12237 0.11238 0.19865 0.20927 0.025123 0.060595 0.027309 0.12286 0.081567 0.083294 -0.12584 -0.043694 0.23828 -0.085715 -0.14252 0.040747 0.041684 0.073632 0.069946 -0.22112 -0.18895 -0.27016 0.072418 -0.060025 0.087542 0.083872 -0.057839 0.046762 0.096467 -0.0049911 -0.13205 0.083263 0.025661 -0.080762 -0.025979 0.025068 -0.13269 0.23799 -0.045537 -0.032486 -0.17542 -0.07087 -0.0092314 -0.1426 0.044785 0.10764 -0.12267 -0.13936 0.15609 0.036979 -0.1347 -0.15584 -0.098543 -0.27965 0.14849 -0.033076 0.04538 0.060087 -0.084011 0.034899 -0.14898 -0.018047 -0.083266 -0.086712 -0.08873 -0.085134 0.10481 0.05159 -0.075947 -0.10214 -0.11083 0.036904 -0.1026 0.00027061 0.038664 -0.05887 0.036666 0.0094112 0.073761 -0.21233 0.19462 0.05661 -0.13601 -0.084792 -0.13766 0.34431 -0.17804 -0.11636 0.14858 -0.18935 -0.14385 -0.10377 0.053098 -0.09365 -0.18925 0.046252 -0.020399 0.13335 -0.17543 0.07522 -0.10008 0.070459 0.068416 -0.092758 0.069816 -0.27967 0.015422 -0.03089 0.079765 -0.10378 0.084016 -0.1664 0.10691 -0.37024 0.052368 0.0141 -0.099556 -0.15061 0.15734 -0.20084 -0.061202 -0.023403 -0.13609 -0.057525 -0.18298 -0.18643 0.091007 -0.039704 0.17626 0.14199 0.0012106 -0.024418 0.076348 -0.14461 0.079363 -0.1741 0.16792 0.14081 0.092129 0.17539 0.062891 -0.17473 0.046689 0.032393 0.082653 -0.15484 0.14681 0.10614 -0.07633 -0.13708 -0.044246 0.109 -0.12887 0.13806 0.14437 -0.080184 0.062125 0.19078 -0.02029 0.13317 -0.015474 -0.072504 -0.13808 -0.013662 -0.159 -0.20814 0.18404 0.089045 0.064484 0.097708 0.14084 0.13587 0.2162 0.015449 0.13246 -0.049131 0.085921 -0.080149 0.074622 -0.21558 -0.045011 0.060289 0.0099671 0.076789 -0.020187 0.065408 0.017595 -0.22571 0.24442 0.017908 -0.28968 -0.20743 -0.13437 -0.068644 0.16343 -0.044069 \n",
            "la -0.087243 -0.20264 -0.086002 -0.15845 -0.07976 0.24064 -0.024934 0.11356 -0.009319 -0.14861 -0.099604 -0.059645 0.1208 -0.16538 0.11366 0.12032 -0.08945 0.11242 -0.10948 0.012922 0.092902 0.063886 0.15397 0.0064485 0.14183 0.1011 0.045985 -0.089442 -0.025309 0.0029423 -0.097328 -0.0012994 0.045796 -0.26232 -0.0076949 -0.090942 -0.040807 0.01449 -0.21705 0.078391 0.20357 -0.045347 0.082835 -0.15313 -0.025507 -0.19498 0.017177 -0.31642 -0.19756 0.097365 -0.15614 0.035947 -0.0082599 -0.14414 -0.15142 -0.09669 -0.2223 0.1094 0.14114 -0.087859 -0.067507 0.027769 0.021343 0.023135 0.27188 -0.31469 -0.10669 0.11766 -0.014595 0.1009 -0.055172 -0.18244 -0.11653 0.10478 -0.24808 -0.19043 -0.052704 0.059834 0.25937 -0.093189 -0.15898 0.038912 0.14788 -0.053159 0.16869 0.12386 -0.096096 0.10762 -0.11596 -0.10957 0.21477 0.23001 0.13008 0.10349 0.081993 -0.096169 -0.22055 -0.16225 -0.0010616 -0.17916 0.046875 -0.0070787 0.0090538 -0.017289 -0.28942 0.17773 0.23212 -0.20835 0.10403 0.147 0.026384 -0.051521 -0.052588 0.09731 0.022555 0.23481 0.2215 -0.11543 -0.062468 -0.15799 0.077123 0.14735 -0.044037 -0.14469 -0.12982 -0.030683 -0.0033111 0.10866 0.29185 0.04682 -0.00098409 -0.0052571 -0.091079 0.064221 -0.063003 -0.094375 0.048162 -0.21204 -0.038798 0.21844 0.10557 -0.14175 0.044464 0.1293 -0.02879 0.0083234 -0.078902 -0.052331 0.12595 0.031053 0.048523 -0.054238 -0.024698 -0.032973 0.024118 -0.063725 -0.042761 -0.031973 -0.09082 0.236 -0.14276 -0.020618 -0.056492 -0.067096 0.0094872 -0.023221 0.17026 -0.24499 -0.097168 -0.024098 0.17047 0.15451 -0.013958 -0.12616 -0.058871 -0.17824 0.076914 -0.18517 0.060541 0.17827 -0.10135 -0.043738 0.059666 0.10141 -0.0086644 -0.053004 -0.22161 0.076878 0.058065 0.17477 -0.075659 -0.14265 0.002005 -0.15388 -0.28006 0.12438 0.13075 0.086962 -0.059509 0.11403 0.1565 0.079556 -0.1379 -0.011146 0.04614 0.22762 0.26415 -0.056961 -0.050645 0.0046277 -0.025336 -0.074364 -0.097341 -0.01886 -0.10939 -0.079747 0.11299 -0.13196 -0.049792 0.19633 -0.11148 -0.11926 0.027289 -0.088296 -0.068848 0.08968 -0.074461 -0.044752 -0.094387 0.086392 0.15731 -0.2303 0.10234 -0.02761 -0.15307 -0.047508 0.27236 0.12071 0.11643 -0.039195 0.012909 0.069169 0.21449 -0.09682 0.19353 0.1306 -0.068394 0.024422 0.043002 -0.22868 -0.080458 -0.0021369 -0.069716 0.081719 -0.058182 -0.0002466 -0.12725 0.050906 0.0025211 -0.16473 0.25295 0.078164 0.10624 -0.0339 0.062989 0.013516 -0.13056 0.28315 0.011068 -0.39159 0.049861 0.070111 0.22991 0.23663 0.15684 0.051742 0.26008 0.08846 0.014493 -0.018517 0.14881 0.086232 -0.27337 -0.082298 0.16161 0.067818 -0.029052 0.078524 -0.011967 -0.078333 -0.23465 -0.049609 0.19376 -0.13847 -0.46039 -0.038072 0.11385 -0.097904 0.057656 0.072709 \n",
            "en -0.07545 -0.038884 0.038438 -0.10724 0.035105 0.06995 -0.0026379 0.041018 0.039145 -0.18308 -0.044143 -0.1012 -0.030684 -0.16878 0.10209 0.20793 0.086362 -0.062942 -0.14584 -0.047861 0.24844 0.19359 -0.035147 -0.1236 0.028792 -0.044538 0.04348 -0.2649 0.085423 -0.059545 -0.11975 0.11814 -0.097434 -0.13552 0.20005 -0.067928 0.013179 0.19967 -0.08636 0.060831 0.12602 0.014311 0.10387 -0.2125 0.1951 -0.13097 0.050486 -0.36252 -0.036009 0.12687 -0.02351 0.082544 -0.17684 0.057777 0.025268 -0.017796 -0.026146 -0.052064 0.23589 -0.12002 0.18026 -0.169 0.15982 -0.16463 0.15429 -0.19096 -0.14133 0.14612 0.020851 0.1193 -0.14367 0.07898 -0.13477 0.026538 -0.042022 -0.1736 -0.23591 -0.097365 0.29347 0.0090856 -0.0091655 0.18018 0.16703 -0.040164 0.11661 -0.075116 0.076377 0.1414 0.090636 -0.01237 0.078946 0.25551 0.029044 0.14561 0.092251 -0.12397 -0.36303 0.097645 0.12059 -0.11897 0.24051 -0.043773 0.040432 -0.12295 -0.070529 0.18437 0.098217 -0.052073 0.060205 0.076155 0.11441 0.15753 0.12658 0.039158 -0.00045034 0.10172 0.33765 -0.058418 -0.11121 0.044991 0.15462 0.023376 0.058628 0.11216 -0.34501 -0.1652 0.057151 0.051537 0.093998 0.1209 -0.018908 0.015916 0.10229 -0.0056713 -0.077965 -0.023026 -0.10484 -0.11566 0.10483 0.105 0.094622 0.050087 0.026057 0.11276 -0.069095 -0.085112 0.19532 -0.10137 0.22176 -0.067579 0.18228 0.0034325 0.029226 -0.077201 -0.083865 -0.056601 -0.012805 -0.17311 -0.11191 0.031172 -0.066539 0.032996 0.096794 0.20214 -0.040894 -0.069747 -0.011413 -0.15794 -0.052371 -0.00036082 0.06581 0.20804 0.13262 -0.12135 -0.13392 -0.075122 -0.20243 0.018669 -0.12831 0.2124 -0.010075 0.037364 0.15738 -0.080925 0.27323 0.05121 0.021477 -0.056585 -0.016459 0.17912 0.0015558 0.093465 0.070182 -0.17871 -0.13999 -0.031899 0.10258 0.04658 -0.028254 0.13619 0.13662 0.017173 -0.0099263 0.10127 -0.11622 0.0157 0.10396 -0.11627 0.15516 -0.027054 0.14388 0.020393 0.050613 -0.079222 -0.069986 -0.1178 0.085856 -0.38193 -0.040251 0.18738 -0.1405 -0.12713 0.084031 -0.20218 -0.11562 -0.011318 -0.17094 -0.17338 -0.23232 0.024059 0.032293 0.12069 0.029851 0.17489 0.078991 0.081048 -0.0087522 -0.049838 -0.037371 0.11619 0.0021115 -0.011379 0.1119 -0.14403 0.27012 -0.10325 0.10284 -0.0032941 0.1114 -0.01124 0.036243 -0.2405 -0.034046 0.012875 -0.057152 0.013018 -0.049557 -0.062849 0.022496 0.014476 -0.028904 -0.1231 -0.061061 0.033134 0.0015246 0.012432 -0.017003 0.080822 -0.14914 -0.34296 0.018496 0.031778 0.22978 0.31816 0.041379 0.074987 0.27721 0.039741 0.091787 -0.040362 0.014706 0.17922 -0.032791 0.052413 0.17985 -0.11966 -0.16692 0.093557 -0.17746 0.067329 -0.24419 -0.033977 0.31626 -0.011872 -0.27428 0.098848 -0.094099 -0.11382 -0.070251 0.20439 \n",
            "el 0.035748 0.07235 -0.069134 0.028064 0.059031 0.07088 -0.072068 0.051085 -0.025811 -0.29631 0.0542 0.10867 0.149 -0.048866 -0.070444 0.028915 0.22319 -0.057101 -0.034669 0.10002 0.17861 0.089148 0.09265 -0.38492 0.13441 -0.15999 0.075882 -0.10702 0.09513 -0.24495 0.1817 0.099435 -0.13467 -0.4172 0.032746 0.21914 -0.14856 0.014076 -0.11198 0.23946 0.20918 0.12006 0.0030198 -0.28722 0.069774 0.039026 0.0026674 -0.1166 -0.042533 -0.11727 -0.16688 0.14746 -0.10435 -0.040089 0.093152 0.1024 -0.037584 -0.093071 0.19807 -0.21302 -0.037371 -0.067561 0.17037 -0.0025929 0.17524 -0.24258 0.18489 -0.043901 0.019096 -0.07971 -0.12906 -0.037951 -0.15561 -0.042212 -0.22028 0.045725 -0.16202 -0.07078 0.29702 0.068473 0.064872 -0.035137 0.0802 -0.00010897 -0.048453 -0.080911 0.029741 0.31707 0.24662 0.0099864 0.28372 0.2084 0.045115 0.2224 0.13213 -0.088049 -0.35532 0.077776 -0.0066971 -0.07158 0.077969 -0.029581 -0.11927 0.014919 -0.075872 0.22144 0.084356 -0.078799 0.022021 -0.050587 0.2259 0.069977 0.010712 0.067618 -0.11129 0.20828 0.087123 0.059309 -0.072418 -0.044649 0.0081396 -0.054189 0.081745 -0.038027 -0.38422 -0.3508 0.027438 -0.20704 0.15623 -0.025075 -0.053779 -0.10121 0.11586 0.069583 -0.19981 0.0090431 -0.0041812 -0.06571 0.16371 0.03994 -0.0095012 -0.050277 0.10041 0.15501 0.015856 -0.0058618 -0.035815 -0.13519 0.066836 -0.12833 0.24889 0.15914 0.1664 0.13836 -0.21555 -0.14942 0.082576 -0.18834 -0.042096 0.14521 0.0062044 0.12418 -0.08792 0.040945 -0.0078187 0.030225 -0.06843 -0.14014 0.040823 -0.1485 0.11026 0.079837 0.063523 -0.050382 -0.19309 -0.13596 -0.078349 -0.042904 -0.099903 0.15535 -0.10639 -0.024307 0.039442 0.045374 0.14995 0.022284 0.074792 -0.13141 0.10031 0.099632 -0.003486 0.063597 0.079861 -0.12305 -0.022075 -0.18952 0.052067 -0.055808 0.020944 0.095219 -0.0027468 0.043778 0.086774 0.1878 -0.03204 0.028848 -0.00011446 0.049086 -0.03336 -0.18719 -0.13072 -0.044743 0.083315 -0.15886 -0.090432 -0.070203 0.14057 -0.11064 0.13378 0.057531 -0.16222 -0.18437 -0.016271 -0.17922 0.073831 0.060655 -0.20236 -0.091576 0.038371 0.062098 -0.0012045 0.11982 0.080371 0.14165 0.062362 -0.036278 0.18981 0.057631 -0.12266 0.0051722 -0.12201 0.02619 0.224 -0.22045 0.18852 -0.16477 0.041919 0.075526 0.16889 0.011214 -0.17314 -0.090979 0.067692 0.085565 -0.104 0.00433 -0.11287 0.0066919 0.0081971 -0.074108 -0.084247 -0.080576 0.010301 0.10869 0.049049 0.11805 0.012963 0.1526 -0.15902 -0.26869 -0.019827 -0.0080557 0.12086 0.31507 -0.019282 0.045085 0.21956 0.10168 0.10081 -0.029663 0.063331 0.24992 -0.028832 -0.068594 0.15222 -0.073604 0.10371 -0.020285 -0.1295 0.11394 -0.024301 -0.23792 0.21563 0.069312 -0.15989 -0.067915 0.0024529 -0.065023 -0.055356 -0.098119 \n",
            "y -0.032596 -0.045742 -0.065643 0.015506 0.12801 0.17722 0.0087185 0.042578 0.061086 -0.30909 0.051332 0.02847 0.11089 0.026909 0.018673 0.13696 0.076586 -0.06908 -0.014897 -0.00054897 0.012264 0.06028 -0.098423 -0.14296 0.048658 -0.002026 -0.10121 -0.22557 0.10547 -0.056413 0.034365 0.065657 0.02634 -0.15916 0.11445 0.13499 -0.095081 0.12763 -0.073913 0.043575 0.06547 0.10495 -0.037375 -0.014293 0.12558 -0.026631 0.18348 -0.1735 0.014401 0.11602 -0.023382 0.049352 -0.16939 -0.093096 -0.16163 -0.020542 0.011027 -0.013327 0.23367 -0.084986 -0.095106 -0.097573 0.06487 -0.028549 0.031747 -0.07378 0.037195 0.12664 -0.070544 0.18386 0.11214 -0.11742 -0.14396 0.068394 -0.035179 0.079584 -0.12191 -0.075848 0.24266 0.060953 -0.068302 0.12417 0.23276 -0.059747 0.073891 0.12044 -0.16861 0.12548 0.097368 0.1194 0.17856 0.11417 0.023018 0.057102 -0.011668 -0.065887 -0.21651 0.031054 0.14228 0.0017269 0.12548 -0.12042 0.11758 -0.034166 -0.12815 0.36861 0.16183 -0.084923 -0.063113 0.00112 0.14095 0.087449 0.19152 0.048646 -0.27184 0.1134 0.093435 -0.038884 -0.11967 0.01686 -0.18419 0.064814 0.14764 -0.17661 -0.29468 -0.15258 0.092621 0.029109 0.19907 0.010692 -0.083357 0.024259 0.11201 0.099902 0.064501 0.015837 -0.0034275 -0.069255 0.040677 0.09464 -0.08387 0.21494 0.060205 0.086762 0.07998 0.01023 -0.04807 -0.053201 0.11513 0.12836 0.13475 0.056281 0.0037868 -0.013353 -0.077463 -0.035613 -0.17316 -0.030994 0.051278 -0.029981 0.049138 0.16231 -0.046761 0.046481 0.098679 -0.15905 -0.085395 -0.04984 -0.085358 -0.028026 0.21624 0.085289 0.0059721 -0.021036 0.0094531 -0.14697 -0.036248 -0.052967 -0.088117 0.095302 -0.090301 -0.12923 0.041615 -0.16869 0.049897 0.036146 -0.010033 0.035531 -0.015741 0.1415 -0.0037263 -0.049178 0.16368 -0.11242 -0.13737 0.076688 0.081923 -0.11212 -0.081697 0.082311 -0.034715 0.0047439 -0.015779 0.094147 -0.093969 0.18186 0.041119 -0.039725 -0.0092453 -0.055285 -0.074539 0.0015384 -0.11958 -0.059766 0.079423 -0.089374 0.11694 -0.16628 0.13485 -0.13099 -0.023501 -0.11466 0.06142 -0.046824 0.019577 -0.053148 -0.13011 0.025236 -0.0065595 -0.14397 -0.010691 -0.12399 0.18993 0.11897 -0.045446 0.057646 0.14989 0.09002 -0.03335 -0.058746 0.02754 0.0068906 0.088286 0.013162 -0.02108 0.0075354 0.018196 0.0057817 -0.036145 -0.038998 -0.013673 -0.077766 -0.10215 -0.0108 -0.12351 0.15285 -0.18851 0.026505 0.076309 -0.063028 0.073936 0.096185 0.089546 -0.079923 0.029956 0.092307 -0.087066 0.17016 -0.15244 -0.28482 -0.0029281 0.097939 0.013741 0.28821 0.14883 0.079594 0.052968 -0.0079471 0.1835 -0.055136 0.018303 -0.09278 -0.089465 -0.044238 0.092031 0.054745 0.076823 0.16024 -0.15864 0.071727 -0.17399 -0.14855 0.071987 -0.035747 -0.34835 -0.19355 0.0017566 -0.19313 0.16428 0.025434 \n",
            "- 0.061882 -0.2376 -0.043609 -0.32502 0.17003 0.40012 -0.35921 0.013048 -0.12689 0.13383 -0.01439 0.030164 0.13667 -0.12514 -0.22801 0.23133 0.16652 0.065258 -0.060323 0.34923 0.43216 0.20237 -0.083912 -0.22783 0.1276 0.077393 -0.039449 -0.10438 0.063382 0.3563 -0.12332 -0.042778 0.047927 -0.17214 0.074145 0.063591 -0.11985 -0.03036 -0.14419 0.090816 0.15342 -0.022861 0.094493 -0.054012 0.18633 -0.34727 0.0014791 -0.01182 0.040599 -0.32866 0.060736 -0.13548 -0.2 -0.035283 0.07056 -0.042737 0.018752 -0.145 0.29329 0.035289 0.16844 0.0011773 0.18096 0.044047 0.042062 -0.047811 -0.03091 0.053162 -0.00010481 0.23463 -0.15055 0.24542 -0.19164 -0.065868 -0.097498 -0.14585 -0.10152 -0.17859 0.23321 0.22965 0.027681 0.028704 -0.037009 -0.019333 0.071626 0.084384 -0.085545 0.18733 0.020018 -0.084429 0.59376 0.10525 0.043524 0.084613 -0.066652 -0.12292 -0.25186 0.20179 -0.050097 -0.14264 0.17217 -0.082422 -0.0006153 -0.065 -0.4059 -0.16767 0.16089 -0.029696 0.16912 0.40095 0.45839 -0.014279 -0.11639 0.071256 -0.037997 0.11131 0.08567 -0.43106 0.29915 0.11901 -0.25041 0.11948 0.046262 -0.12303 -0.17653 -0.17842 -0.18363 -0.22566 0.09684 -0.17136 0.24874 0.020866 0.14425 0.066565 -0.4463 0.11817 0.046838 -0.5492 0.27733 0.21647 -0.27258 0.23835 -0.203 -0.013014 -0.21551 -0.008156 0.34472 -0.38538 -0.50106 -0.074358 -0.028768 0.00051442 0.023616 0.049602 -0.053997 0.093219 -0.21057 -0.048011 0.21441 -0.058004 0.022001 0.16849 0.017839 0.059289 0.016691 -0.10045 0.10477 0.1238 -0.36308 -0.33255 -0.18506 0.14948 0.10076 -0.095559 0.28059 0.10842 -0.013423 -0.026027 -0.12341 -0.24032 0.21235 0.02712 0.12364 -0.21852 0.42956 0.16091 -0.19935 0.1226 -0.40844 0.15508 -0.28825 -0.082169 -0.084684 -0.32858 -0.2214 -0.076531 -0.29772 -0.29366 0.052511 0.002251 0.19767 -0.0062039 0.12849 0.10865 -0.12288 -0.0011042 -0.16796 -0.22486 -0.12273 -0.23674 0.022002 -0.16565 0.17226 -0.15764 0.13493 -0.17136 -0.097008 -0.10482 -0.05266 -0.47761 -0.099342 -0.39334 0.11196 -0.12544 -0.11805 -0.091981 -0.032006 -0.028742 -0.004083 -0.33484 -0.20864 -0.090755 0.21117 0.24573 0.10072 0.13941 0.079793 0.21366 0.30519 -0.32969 -0.035839 -0.041737 0.068225 0.42737 0.089181 0.16726 -0.067511 0.026349 0.19177 0.012491 -0.24549 0.13706 -0.082386 0.18589 0.15824 0.093463 -0.017767 -0.0037602 -0.15217 -0.16721 0.20758 0.13573 0.30144 0.20296 -0.01547 -0.033869 -0.010441 -0.061908 -0.43752 0.051394 -0.006136 0.18271 -0.0002473 0.20526 0.2277 0.34907 0.24501 -0.2586 0.16812 -0.069779 -0.29855 0.09952 0.13152 0.093342 0.14696 0.24357 -0.092526 0.53126 -0.13048 -0.053975 -0.0096316 -0.20051 0.10871 -0.028493 -0.23259 -0.52925 0.045869 -0.016944 0.030856 -0.090491 \n"
          ]
        }
      ],
      "source": [
        "!head wiki.es.vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nV8ccla1N0f"
      },
      "source": [
        "La carga del modelo es un poco distinta a lo visto anteriormente. En concreto debemos usar el siguiente código (notar que no cargamos todo el documento sino solo los 100000 primeros n-gramas). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "WA7dcPDE1tSp"
      },
      "outputs": [],
      "source": [
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "wordvectors_file_vec = 'wiki.es.vec'\n",
        "cantidad = 100000\n",
        "wvFastTextSpanish = KeyedVectors.load_word2vec_format(wordvectors_file_vec, limit=cantidad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARM9aM5u2Bbr"
      },
      "source": [
        "Ahora podemos utilizar las mismas funciones vistas anteriormente. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rg5LIf9a2F5f"
      },
      "source": [
        "**Ejercicio:** Encuentra las 5 palabras más similares a bicicleta. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "maVFQ3Z72UF9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "006e99c3-63dd-4421-c0e8-d8c254d0abf1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('bicicletas', 0.8180868029594421), ('bici', 0.7310570478439331), ('motocicleta', 0.6779677867889404), ('bike', 0.6354870796203613), ('moto', 0.5987207889556885)]\n"
          ]
        }
      ],
      "source": [
        "print(wvFastTextSpanish.most_similar(positive=['bicicleta'], topn=5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQEsWYJC2T7J"
      },
      "source": [
        "**Ejercicio:** Responde a la siguiente analogía. *Hombre es a actor como mujer es a...*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Q7HlAcbh2TqW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c98a8d1-0b4c-4896-ab64-18a8ce2e3d7b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('actriz', 1.0298713445663452),\n",
              " ('bailarina', 0.9005751013755798),\n",
              " ('actrices', 0.900486409664154),\n",
              " ('dramaturga', 0.8913683891296387),\n",
              " ('coreógrafa', 0.884644627571106),\n",
              " ('locutora', 0.8787722587585449),\n",
              " ('presentadora', 0.8759288787841797),\n",
              " ('compositora', 0.8649977445602417),\n",
              " ('actoral', 0.8637272119522095),\n",
              " ('comediante', 0.8625256419181824)]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "wvFastTextSpanish.most_similar_cosmul(positive=['actor','mujer'],negative=['hombre'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgD-aD_N2U2V"
      },
      "source": [
        "**Ejercicio:** Responde a la siguiente analogía. *Canta es a cantar como juega es a ...*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "A-eRyX_d2VM6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "831f6b36-ecaf-421d-b08e-c73c51d0f82f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('jugar', 0.9471045136451721),\n",
              " ('jugaba', 0.87227863073349),\n",
              " ('centrocampista', 0.8656566739082336),\n",
              " ('jugando', 0.8536456227302551),\n",
              " ('mediocampista', 0.8485172986984253),\n",
              " ('mediapunta', 0.8384414315223694),\n",
              " ('jugó', 0.837324857711792),\n",
              " ('jugarse', 0.8344488739967346),\n",
              " ('juegue', 0.8287681937217712),\n",
              " ('jugara', 0.827880859375)]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "wvFastTextSpanish.most_similar_cosmul(positive=['cantar','juega'],negative=['canta'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bb3xAmT2Vlp"
      },
      "source": [
        "**Ejercicio:** Responde a la siguiente analogía. *Madrid es a España como Lisboa es a ...*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "RTyWCrCY2WPW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14a604e6-53d5-47e3-a0ed-cc976230c0f1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('portugal', 0.9711438417434692),\n",
              " ('algarve', 0.872070848941803),\n",
              " ('estêvão', 0.8418089747428894),\n",
              " ('portuguesas', 0.8351491689682007),\n",
              " ('portuguesa', 0.8348497152328491),\n",
              " ('alentejo', 0.8336402177810669),\n",
              " ('portimão', 0.8250821232795715),\n",
              " ('portugues', 0.8246365189552307),\n",
              " ('portugueses', 0.8224292993545532),\n",
              " ('brasil', 0.8218298554420471)]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "wvFastTextSpanish.most_similar_cosmul(positive=['españa','lisboa'],negative=['madrid'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFQ1GuhQ3AaS"
      },
      "source": [
        "**Ejercicio:** Encuentra la palabra que no encaja en la lista ``[lunes, martes, septiembre, jueves, viernes]``"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "6o9_iLyx3AIz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "ce25532c-48f3-4b16-eb05-0fd8d89d3585"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py:895: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'septiembre'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "wvFastTextSpanish.doesnt_match(['lunes', 'martes', 'septiembre', 'jueves', 'viernes'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZUZ0WBN3NRQ"
      },
      "source": [
        "**Ejercicio:** Descarga alguno de los otros modelos proporcionados en el [zoo de modelos](https://github.com/dccuchile/spanish-word-embeddings) y encuentra un ejemplo de analogía y otro de palabra que no encaje usando dicho modelo (añade tantas celdas como necesites)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Nkw_gJnx3eY5",
        "outputId": "32a6523e-4089-4249-db77-7ff2cb4af177",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-25 13:42:11--  https://zenodo.org/record/3234051/files/embeddings-l-model.vec?download=1\n",
            "Resolving zenodo.org (zenodo.org)... 137.138.76.77\n",
            "Connecting to zenodo.org (zenodo.org)|137.138.76.77|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3446609027 (3.2G) [application/octet-stream]\n",
            "Saving to: ‘unnotated.es.vec’\n",
            "\n",
            "unnotated.es.vec    100%[===================>]   3.21G  19.1MB/s    in 5m 56s  \n",
            "\n",
            "2022-05-25 13:48:08 (9.24 MB/s) - ‘unnotated.es.vec’ saved [3446609027/3446609027]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://zenodo.org/record/3234051/files/embeddings-l-model.vec?download=1 -O unnotated.es.vec"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head unnotated.es.vec"
      ],
      "metadata": {
        "id": "uUyksdU7ZCNB",
        "outputId": "b469ed27-b5d2-443f-f7c3-c22ebeed14c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1313423 300\n",
            "</s> -0.16791 -0.22665 0.078156 -0.18347 0.21301 0.055032 0.067502 0.13706 -0.14041 0.19155 -0.13608 0.040696 -0.036988 -0.099929 0.059987 -0.006037 0.07959 -0.086929 0.0032061 0.04053 -0.092542 0.086853 0.11532 0.15845 0.078315 -0.17212 -0.1129 -0.14736 -0.031068 0.10991 -0.21799 -0.064893 -0.061097 -0.015511 -0.09624 0.062328 -0.013588 -0.005091 -0.12687 0.088444 -0.17059 -0.010566 0.13244 -0.23103 -0.0057486 -0.031596 -0.011091 -0.015354 -0.022998 0.13553 0.25073 -0.0090087 0.048081 -0.022228 -0.050962 -0.064517 -0.11814 0.27844 -0.021293 -0.18498 0.15431 0.14207 0.053311 -0.091351 -0.17444 -0.028598 -0.067873 0.054908 -0.018664 -0.13587 0.011815 -0.13598 -0.10257 0.29452 -0.038179 -0.18362 0.045844 0.075999 -0.074984 -0.10918 -0.00094654 0.04368 -0.01443 0.019339 -0.093375 -0.11623 0.013624 -0.19358 0.052397 0.044992 0.058078 -0.033469 0.1245 -0.13038 -0.046593 0.032381 0.047781 -0.016469 0.004409 -0.037789 -0.13712 0.16659 -0.062807 0.28074 -0.04625 0.0022809 0.016488 0.19926 0.11172 -0.03427 0.14416 0.053243 -0.09638 0.058931 0.075612 -0.065511 -0.0060492 0.020789 -0.13386 0.052195 -0.052739 0.19476 0.038455 -0.10598 -0.18337 -0.058846 -0.13525 -0.1222 0.051422 0.076732 0.1402 0.12934 -0.052591 0.0084305 0.095921 0.10526 -0.070922 -0.18238 -0.085352 -0.053372 0.085621 0.061676 0.018952 0.011765 0.081596 0.082155 0.088519 0.013253 -0.083824 -0.013324 -0.007205 0.061117 0.10587 0.17834 -0.14557 0.077975 -0.044804 0.056764 -0.027812 -0.027976 0.10195 0.15638 0.087823 0.1417 -0.18691 -0.052343 -0.081719 0.0088605 -0.00085984 0.036773 -0.076224 0.065761 0.13338 -0.022946 0.094836 -0.33265 -0.20515 -0.103 0.070352 0.011667 0.1037 -0.072217 0.21567 -0.01147 0.16768 0.090789 -0.072345 -0.10506 -0.12249 -0.093792 -0.032502 -0.19983 -0.14351 0.068827 -0.0092882 0.037769 -0.095185 -0.013311 0.19932 -0.013811 -0.052147 0.10024 0.03652 0.15711 0.047173 0.12175 -0.19109 0.054299 -0.15652 -0.15103 -0.0063821 0.12243 -0.06128 0.14071 -0.018398 -0.028478 0.039067 -0.093989 0.12158 -0.18514 0.01874 0.21111 0.086176 -0.034578 0.27032 0.11894 0.042813 -0.0077885 -0.10076 0.11451 0.0050129 -0.23467 0.17028 0.043155 -0.14286 -0.07089 -0.011192 0.12469 0.13269 0.049463 0.17032 -0.01566 0.020772 0.020939 -0.18796 -0.095613 -0.079231 0.12745 -0.10848 -0.11315 -0.11356 0.061496 0.06361 -0.033958 -0.034258 0.015434 -0.073679 -0.034736 0.089546 0.016684 0.044723 -0.28364 0.05975 -0.038468 0.14002 0.15364 -0.0024464 0.0025992 -0.049165 0.069657 -0.087388 -0.060743 -0.0427 -0.14385 -0.16666 -0.0076047 0.055074 0.20637 -0.059712 0.067797 0.026987 -0.12592 -0.1367 -0.0049239 -0.031696 -0.023073 0.0073785 0.14925 -0.0098459 -0.047273 -0.12459 0.25998 -0.00013614 0.09669 0.031655 0.007589 0.068359 0.0053576 0.094109 -0.092816 \n",
            ". -0.14761 -0.24194 0.086593 -0.23465 0.30719 0.0022006 0.16329 0.21969 -0.10601 0.17329 -0.19975 0.068552 0.06077 -0.13768 0.084967 -0.039105 0.08131 -0.040073 0.017582 0.01896 -0.06401 -0.0097301 0.12749 0.070524 0.11639 -0.18678 -0.08364 -0.17844 -0.078954 0.12204 -0.15599 -0.03742 -0.12933 0.053842 -0.0031257 0.16299 -0.038735 -0.04622 -0.18736 0.023712 -0.2074 0.039148 0.060108 -0.18355 0.034211 -0.00073051 -0.056355 0.067801 0.016163 0.17298 0.1232 -0.002583 0.020565 -0.054498 -0.071533 0.032839 -0.12536 0.31473 0.017446 -0.29006 0.1388 0.18281 0.045323 -0.1496 -0.13288 -0.10028 -0.15268 0.062887 0.013852 -0.095012 0.028591 -0.079505 -0.076861 0.29143 -0.0728 -0.16495 0.010383 0.073661 -0.076483 -0.057705 -0.054562 0.028044 0.11115 -0.0076513 -0.17457 -0.077213 0.0075379 -0.1561 0.00053507 0.059926 0.085118 -0.035202 0.11993 -0.097897 -0.064098 0.042715 -0.010939 0.0054071 0.076078 -0.090835 -0.075065 0.234 -0.021077 0.33068 -0.11121 -0.092623 0.030812 0.11874 0.14812 -0.013614 0.087795 0.068003 -0.072816 0.028295 0.18047 -0.02735 2.7627e-05 0.096066 -0.12825 0.039571 -0.040899 0.10587 -0.020502 -0.099622 -0.13894 -0.18417 -0.14346 -0.024736 -0.018516 0.17427 0.27037 0.092548 -0.0020838 0.087091 0.046633 0.068376 -0.045902 -0.093758 -0.11316 0.028347 0.094303 0.07384 -0.058289 0.02572 0.13956 0.07498 0.12408 -0.015496 -0.12658 -0.015348 0.02352 0.080623 0.082374 0.12408 -0.12598 0.019505 -0.046907 0.033189 0.030032 -0.0077958 0.050007 0.20765 0.11504 0.17882 -0.23762 -0.0965 -0.064223 -0.062394 0.052499 -0.012821 -0.034932 0.075276 0.029402 -0.032669 0.14885 -0.33468 -0.17776 -0.081346 0.02135 0.12621 0.070478 -0.10788 0.14456 -0.053796 0.11207 0.0257 -0.078106 -0.016589 -0.15586 -0.20409 -0.059952 -0.19603 -0.099419 0.094344 -0.048858 0.019912 -0.12377 -0.044085 0.15265 0.018986 -0.049391 0.14412 0.043378 0.072866 -0.039479 0.16826 -0.15317 -0.021152 -0.19958 -0.12208 -0.083744 0.22589 -0.018511 0.14686 -0.13906 -0.061679 0.024993 -0.046684 0.17543 -0.16482 -0.005084 0.1662 -0.069133 -0.015898 0.29157 0.10033 0.045913 0.017904 -0.097307 0.14889 0.014619 -0.10452 0.10094 -0.0089898 -0.16098 -0.035008 -0.002623 0.029482 0.06168 -0.00055021 0.27795 -0.021376 0.038526 0.028599 -0.26797 -0.10545 -0.14471 0.056011 -0.12095 -0.030852 -0.092757 0.087255 0.097902 0.0511 -0.0027051 -0.013629 -0.088667 -0.05678 0.012372 -0.016023 0.024246 -0.26682 0.072428 -0.070094 0.026648 0.13962 -0.011749 0.13593 0.027322 0.11717 -0.13299 0.038794 -0.091363 -0.12771 -0.12886 -0.026511 0.072779 0.20797 -0.0028858 0.15795 0.068429 -0.062987 -0.14902 -0.001165 -0.08 -0.06461 -0.10268 0.22134 0.027634 -0.078503 -0.13956 0.23899 -0.046659 0.13925 -0.046009 0.064353 0.13863 0.0012655 0.0092182 -0.073035 \n",
            "de -0.10702 -0.16535 -0.023118 -0.12705 0.2112 0.056415 0.26149 0.13142 -0.11482 0.065947 -0.012686 0.012555 0.077445 0.034699 -0.057658 0.06128 -0.10333 -0.13932 0.0023533 0.098542 0.0099296 0.039229 0.16205 0.025763 -0.051084 -0.16917 -0.14543 -0.00072604 0.065243 0.32765 -0.10585 0.041143 -0.059614 -0.14914 0.11296 -0.29306 0.027254 -0.082432 0.01474 0.16256 -0.2072 0.28676 0.15164 -0.029683 -0.012216 0.13447 -0.034875 -0.018221 0.083328 0.083921 0.089639 0.069744 0.19979 -0.035026 0.053043 0.040025 -0.14216 -0.0087685 0.13036 0.099687 0.02752 0.018683 0.19605 0.031515 -0.054066 -0.086599 -0.023598 -0.015026 0.05799 0.073724 0.063729 -0.01949 -0.1264 0.12285 0.00047743 -0.014645 0.0084977 0.10523 0.02203 0.0032143 -0.13342 -0.063385 -0.20137 -0.047274 -0.056007 0.0069563 0.010402 -0.21691 0.12015 0.057138 -0.20341 0.26469 0.0057118 0.056803 0.18587 -0.033444 0.060802 -0.11163 0.11905 -0.1632 -0.18995 0.17463 -0.017873 0.26618 -0.23277 -0.13329 0.1575 -0.027295 -0.086283 0.35444 0.03999 0.36901 -0.036582 -0.021965 -0.11441 -0.046905 0.0089712 -0.0068035 0.046768 -0.024473 -0.1102 0.028275 0.0044647 -0.20854 0.023353 -0.10076 0.056013 -0.34955 -0.0084313 -0.049048 0.076348 0.019991 0.14973 -0.032977 -0.12482 0.03629 -0.2664 -0.060181 -0.05223 0.035488 0.011082 0.046345 -0.049841 -0.027192 0.068258 0.08645 0.028039 -0.12828 -0.10308 -0.10025 0.18326 0.00093478 0.19621 0.19115 -0.068335 -0.01949 -0.01204 -0.19235 0.008496 -0.092191 0.13321 0.086656 -0.0058693 -0.064694 0.012969 -0.081713 -0.069571 0.13593 -0.16033 -0.058112 0.029882 -0.048436 0.03212 -0.081605 -0.038345 -0.3139 -0.060366 -0.084813 0.090056 0.085333 0.20629 0.10955 0.04796 -0.065578 0.065868 0.25672 -0.13678 0.064274 0.047935 -0.042375 0.17967 -0.017706 0.029232 -0.15755 0.15829 -0.042314 -0.036596 -0.16193 0.14675 -0.23962 -0.083084 0.022435 0.019989 -0.034386 0.0063884 -0.072184 0.085677 -0.06661 0.019977 0.10923 0.0027875 0.12169 0.085236 0.14393 -0.17269 0.15464 0.053386 -0.11009 0.1017 -0.16248 -0.19891 -0.091727 -0.011734 -0.073825 0.15797 0.14326 -0.019121 0.074976 -0.049975 -0.065184 -0.024609 -0.0087333 0.12543 0.057218 -0.13896 -0.047617 -0.05084 0.17253 0.14081 -0.015422 0.15238 -0.06304 -0.12242 -0.10029 0.070557 -0.011685 -0.1578 -0.058515 -0.070842 -0.10116 -0.1921 0.10264 -0.10124 -0.017377 -0.29264 -0.058156 0.090084 -0.19596 -0.16329 -0.011561 0.14061 0.080958 -0.025872 0.0561 0.15744 0.13666 0.045474 -0.14164 -0.066849 0.043934 0.12336 -0.13733 -0.086769 -0.030184 -0.2218 -0.032158 0.091317 -0.058511 0.073668 0.014296 0.11344 0.046858 -0.14302 0.075019 -0.14936 -0.068247 -0.18296 0.23463 -0.020764 -0.021636 -0.095976 0.10067 -0.0095205 -0.10233 0.0095296 0.026332 0.043856 0.08132 0.25142 0.1581 \n",
            ", 0.0065246 -0.21248 0.050515 -0.11237 0.116 0.12188 0.1296 0.25899 -0.15125 0.10983 -0.08704 0.10031 0.11476 -0.092404 0.017707 0.013908 -0.050764 -0.075855 0.0064614 -0.040084 -0.010287 0.10006 0.073073 0.075918 0.02206 0.029742 0.00062442 -0.1728 -0.060055 0.009254 -0.14295 -0.041424 0.042084 -0.054981 -0.014191 -0.029075 -0.11273 -0.051202 -0.078674 0.11304 -0.18466 0.011404 0.025288 -0.20964 -0.038209 0.052811 0.020008 0.0079594 0.053491 0.093959 0.19219 0.019059 0.098112 -0.10025 -0.10034 -0.03345 -0.053753 0.087502 -0.087597 0.01032 0.12568 0.012268 -0.089029 -0.0034261 -0.15089 0.049315 0.10639 -0.0050259 0.1301 -0.043782 -0.029875 -0.0029789 -0.11897 0.19348 -0.085142 -0.03258 -0.085841 0.12887 0.069562 -0.075153 0.015715 -0.024612 -0.096662 -0.0053382 -0.15171 0.096266 0.054234 -0.16434 0.0011695 0.10168 -0.16965 0.18295 0.016272 -0.06445 0.069527 -0.086575 0.086257 0.069594 0.065426 -0.1054 -0.16291 0.035309 0.0042084 0.26397 -0.04796 -0.081992 0.024182 0.056521 -0.05493 -0.053386 0.077973 -0.00070238 -0.15882 0.2458 -0.032576 0.11026 -0.11983 0.077679 -0.0018191 0.021393 -0.11442 0.12162 0.050591 0.11271 -0.15693 -0.081358 -0.13094 -0.087917 0.060157 0.095857 0.20153 0.2269 -0.030635 -0.11435 -0.064953 0.075365 -0.12086 -0.054653 -0.17771 0.032361 0.026234 -0.0079811 -0.0035369 0.01555 -0.018749 0.077705 0.047509 -0.10392 -0.0097387 0.0012927 0.033228 0.070738 -0.0082063 -0.088965 -0.15606 0.030398 -0.14364 -0.16108 -0.088902 -0.10052 0.082365 0.087585 -0.0025471 0.21851 -0.14365 -0.041469 0.043992 -0.015283 -0.047517 -0.07121 -0.0026975 -0.063344 -0.015216 -5.0545e-05 0.015241 -0.36219 -0.025495 -0.12201 0.15175 0.12226 -0.01187 3.1114e-05 0.058747 -0.023984 0.042776 0.11554 -0.050859 -0.1209 -0.15506 -0.053602 0.015202 -0.18559 -0.097215 -0.040876 0.16206 0.05541 0.033956 -0.054821 0.096712 -0.059708 -0.1721 -0.0034337 -0.0013232 0.21237 -0.082487 0.19049 -0.072484 0.072061 -0.12826 -0.0043251 -0.096354 0.080231 -0.02048 0.23919 -0.062765 0.088162 0.07064 -0.035672 0.10655 -0.1918 -0.081702 0.072329 0.039888 0.034764 0.10855 -0.026873 0.026355 0.022784 -0.094054 0.064706 -0.0061591 -0.12063 0.19365 0.074011 -0.23614 0.0096831 -0.080677 0.16813 0.13614 0.01132 0.10286 0.067889 0.036195 0.042825 -0.14894 0.018334 -0.17556 0.063724 -0.091141 0.04535 0.032094 0.13152 0.083389 0.018834 -0.078535 -0.0239 0.086349 -0.068556 -0.0073245 -0.084611 0.038138 -0.0041931 0.039551 0.054981 0.17308 0.091101 -0.023107 -0.078597 0.067598 0.066002 0.0066168 0.051262 -0.057764 0.081273 -0.050345 -0.23104 0.047886 0.060413 -0.0017965 0.07887 -0.072138 -0.17234 -0.079312 -0.047772 -0.044228 -0.22799 -0.21507 0.17294 0.05734 -0.056534 -0.1998 0.17522 -0.013625 0.099579 -0.071879 0.020205 0.042195 0.061888 -0.053148 -0.03101 \n",
            "la -0.10727 -0.13511 -0.12576 -0.21309 0.14979 0.06417 0.22174 0.10193 0.16273 0.25195 0.10151 0.11798 0.11558 -0.14295 -0.068607 0.1574 0.023109 -0.094135 0.012598 0.030987 -0.06156 0.0055115 -0.00078012 0.080115 -0.093592 -0.060831 -0.01223 0.12227 -0.026645 0.1817 -0.13181 0.10728 0.038341 -0.025656 0.018655 -0.058394 0.040503 0.055501 0.033464 -0.018042 -0.024936 0.14159 0.21367 -0.099776 0.022435 0.060737 0.18016 -0.081533 0.039898 0.051434 0.10841 0.037585 0.1161 -0.19458 0.034268 -0.072991 -0.23055 0.16388 0.13436 -0.14827 0.0090261 0.1569 0.18297 -0.052885 -0.20466 0.042054 0.13574 -0.11986 0.09777 0.053692 0.044192 0.062494 -0.10038 0.13282 -0.063802 -0.045908 0.14328 -0.0099415 0.1327 -0.019494 -0.13015 -0.059977 -0.14472 -0.12153 -0.054742 0.23882 0.14016 -0.097282 0.034718 -0.027054 -0.11239 0.31448 0.0081642 -0.078411 0.10942 0.064846 -0.056656 -0.10081 0.13389 0.057336 -0.108 0.2588 -0.020753 0.33607 -0.21678 -0.022428 -0.036826 0.16181 -0.24867 0.14683 0.28012 0.12804 -0.18966 0.095248 -0.172 0.071479 -0.12536 -0.024252 -0.050677 0.07979 -0.032166 0.12071 -0.064279 -0.090355 -0.1352 0.11064 -0.018271 -0.0072029 -0.15401 -0.27477 0.046669 0.19639 0.19519 0.029481 0.068519 -0.059141 -0.2286 -0.29713 -0.081193 0.06928 -0.0036889 0.17022 0.08595 -0.21043 0.13011 -0.014179 0.078068 -0.11501 -0.1195 0.018696 0.071302 0.11057 -0.049657 0.21384 0.051041 0.03305 0.10347 -0.10968 0.04915 -0.032034 0.167 0.068914 0.08529 -0.15197 -0.02235 -0.19441 0.10594 0.2134 -0.07095 0.030042 0.02916 -0.034061 -0.0059217 0.019037 -0.0034994 -0.012253 0.10063 -0.2378 0.17402 0.16584 0.12528 -0.037512 -0.11349 -0.10291 -0.13856 0.30894 -0.21137 -0.11646 0.068625 -0.028491 -0.029289 0.096639 -0.081784 -0.077951 0.083379 -0.050141 -0.035418 -0.10986 0.14294 -0.32046 0.10119 -0.27176 -0.0051754 -0.15457 -0.10634 0.048946 0.015845 0.15518 -0.11599 0.15277 0.027056 0.19413 -0.0052931 0.033267 -0.14496 0.081719 -0.14221 -0.17439 0.047529 -0.12515 0.22901 0.015175 0.078861 -0.099695 -0.072226 0.093151 0.075344 -0.02169 -0.083172 -0.0653 -0.049755 0.0033498 0.10002 0.25125 -0.080759 0.070852 -0.17246 0.18353 0.28631 -0.112 0.1074 0.07393 -0.094729 -0.020038 -0.073715 0.098692 -0.11244 0.044724 -0.24514 -0.10515 0.12143 0.17038 -0.11631 -0.049832 -0.22222 0.052534 0.1641 -0.051973 -0.15899 -0.018437 0.09352 0.17192 -0.087517 0.11381 0.22324 0.084241 -0.10568 -0.12847 0.023347 -0.18252 0.18545 -0.039311 -0.10865 0.18897 -0.26886 0.0095554 0.035739 -0.20229 0.02967 0.16901 -0.038606 0.019821 -0.16626 0.010475 -0.06908 -0.025119 -0.13908 0.2172 -0.17631 -0.044678 -0.048895 0.15716 -0.08246 -0.070826 0.077295 -0.017452 0.13174 0.12981 0.21136 0.035919 \n",
            "que -0.030299 -0.29811 -0.0062378 -0.027126 0.15442 -0.051471 0.041579 0.23255 -0.0052058 0.10965 0.040777 0.11879 0.016175 0.0297 -0.061437 0.19265 -0.057526 -0.13851 0.11349 -0.0027214 0.10244 -0.0098301 0.013964 0.098224 0.014675 -0.016653 -0.11921 -0.05955 0.12457 -0.18247 -0.14566 -0.0061381 0.11645 0.015331 0.14016 -0.07544 -0.053159 0.21371 -0.072698 0.14627 -0.07756 -0.047412 0.089439 -0.14947 -0.15606 0.12729 -0.040339 0.21319 0.12088 0.010317 0.03527 -0.054205 0.17034 -0.085028 -0.05016 -0.12489 0.065845 0.22287 -0.053746 -0.081712 0.24134 0.13462 -0.18339 -0.19293 -0.012349 0.10382 -0.020826 -0.055506 0.098522 -0.11087 -0.085275 0.11348 -0.10347 0.097735 -0.1493 -0.091279 0.049897 -0.097307 0.11353 0.13835 -0.084987 -0.28126 -0.051113 0.24354 -0.22259 0.10834 -0.074457 0.083304 -0.15098 -0.16512 0.079153 0.099536 -0.12925 -0.11428 0.012628 -0.037987 0.043901 -0.055162 0.014011 -0.04263 0.044567 0.09447 0.10586 0.070918 -0.14377 0.086091 0.104 0.21418 0.19893 -0.07211 -0.01072 -0.024458 -0.059194 -0.0090315 0.058321 -0.058236 -0.022119 0.022566 -0.052136 0.037249 0.04709 -0.058759 -0.013141 -0.07559 -0.016521 -0.034647 0.050551 -0.14348 -0.036245 -0.10396 -0.071815 0.062633 -0.066217 0.018165 -0.020833 0.043367 -0.18285 -0.23302 -0.076956 0.081562 0.012208 0.094807 0.05613 0.097676 -0.14779 0.081906 0.0041445 -0.17469 -0.0013545 0.13607 -0.012188 0.01438 -0.016368 -0.028666 0.0090012 0.03446 -0.1298 0.042287 0.024421 0.048975 -0.12758 0.11481 0.11912 0.1152 -0.12349 -0.02008 0.26866 0.073671 0.059173 -0.1071 0.239 0.014571 0.011711 0.05706 -0.074515 -0.16522 0.068795 0.04673 0.055951 0.019321 0.051564 -0.10271 0.20111 -0.12339 -0.014353 0.01404 -0.11548 -0.022894 -0.073673 -0.023726 0.035253 0.17246 -0.078303 -0.18835 0.16233 0.032262 -0.089958 -0.0069289 0.17025 -0.22204 0.00026904 0.16648 0.074003 -0.10722 0.066607 0.11947 0.072497 -0.021191 -0.096344 0.019932 -0.13433 0.044643 0.061903 0.13209 -0.22787 -0.015109 -0.1342 -0.068367 -0.031363 -0.0046797 0.12823 0.0058401 0.085561 0.079041 0.21493 -0.13511 0.07287 0.0331 -0.00087898 -0.014586 -0.077483 0.10836 0.31097 0.015065 -0.18458 -0.11842 -0.20799 0.32653 0.29452 -0.12631 -0.038241 -0.023102 -0.046471 -0.16509 -0.061898 -0.0085802 -0.11019 0.073298 -0.066276 -0.056571 0.080921 0.12323 -0.043899 0.074749 0.088517 0.01772 0.020465 0.0073308 -0.0054723 -0.013274 -0.0077046 -0.026471 0.15981 0.032404 -0.06535 0.14111 0.028639 -0.075757 0.29205 -0.010452 -0.062493 0.042158 -0.08189 0.11614 -0.10526 0.15478 -0.059074 -0.15391 0.015636 0.15479 0.18718 -0.1474 -0.2491 0.026525 -0.21879 -0.045711 -0.058347 0.12685 -0.081836 0.15634 -0.18822 -0.0020224 0.00050833 0.0089018 -0.11508 -0.022657 -0.015773 0.15349 0.15341 -0.011805 \n",
            "el -0.23316 -0.30023 -0.078837 -0.031105 0.24113 0.048195 -0.16432 0.079014 -0.1199 0.19593 -0.1578 0.12034 0.12264 -0.32424 -0.038912 -0.098194 -0.05464 -0.25311 0.064098 -0.051447 0.13664 0.080641 0.15479 0.024851 -0.011947 -0.10756 -0.061465 -0.14045 -0.037595 0.14363 -0.26828 0.079142 -0.0039375 0.15922 0.21199 -0.071387 -0.1759 0.020752 -0.047598 0.22663 -0.21218 -0.037462 -0.12042 -0.1637 -0.10613 0.10189 0.086745 -0.047254 0.035489 0.10398 0.12942 -0.021746 -0.19857 0.23729 -0.014425 0.092834 -0.19755 0.086468 0.12341 0.038495 -0.031603 -0.025443 -0.082814 -0.067426 -0.14562 0.11574 0.16743 -0.048233 0.087624 0.091382 0.063259 -0.024311 -0.34438 -0.011502 -0.050843 -0.12194 -0.0017037 0.06526 0.046068 -0.030149 -0.032272 -0.10379 -0.29755 0.15755 -0.23325 0.10205 -0.097924 0.025473 0.089692 0.076247 -0.072264 0.1695 0.036542 -0.025425 -0.054946 -0.21559 0.15055 -0.031908 -0.16528 -0.028464 -0.12253 0.11807 0.060814 0.090076 -0.10238 0.072473 -0.025301 0.15038 0.04614 0.10818 -0.11085 0.040923 -0.021912 -0.034518 -0.22219 -0.01406 0.006855 0.29315 -0.078647 -0.047878 0.1215 -0.018602 -0.095264 -0.15421 -0.14155 -0.097063 0.019008 -0.18038 0.11384 0.11897 -0.11715 0.31155 0.0013484 -0.15371 -0.10039 -0.012219 -0.055967 0.060861 -0.165 -0.078324 0.070588 0.20802 -0.075563 0.073057 -0.12082 -0.051289 0.0029233 -0.11939 0.074615 0.10536 0.15995 0.064332 0.056288 -0.0047807 -0.14182 -0.047339 -0.026496 -0.010399 0.02481 -0.019827 0.072354 -0.12245 0.10742 -0.11041 0.079769 -0.073942 0.15 -0.011045 -0.080907 0.05235 0.22395 0.11381 -0.060974 0.083498 0.0088707 -0.36265 0.04778 0.060813 0.13896 0.068267 0.15699 -0.042369 0.046563 -0.056925 -0.079332 0.22019 -0.40695 0.079594 0.27478 -0.18597 -0.019682 -0.10848 -0.15251 -0.065504 -0.044695 -0.0099664 0.031212 -0.13103 0.084793 0.072754 0.10315 -0.086019 0.1421 0.021294 -0.049132 -0.12613 -0.046553 0.087124 -0.053996 0.21749 -0.18803 0.071847 -0.10294 0.17753 -0.037137 -0.16047 -0.03826 -0.1016 0.16392 -0.3612 0.0096056 -0.035727 0.025066 -0.022907 0.023687 0.072457 0.008896 -0.23995 -0.10045 0.017336 0.0376 0.053825 0.2977 0.070901 0.015707 0.079979 0.098767 0.21618 0.14735 0.011762 0.22058 -0.0074916 0.09067 -0.13243 -0.040811 -0.13517 -0.072745 -0.015596 -0.097119 -0.07981 -0.0085346 0.049172 0.020695 -0.04726 -0.081897 -0.13541 0.24822 -0.056162 -0.18254 -0.13913 0.26378 -0.053277 -0.025408 -0.025275 0.14148 -0.093746 -0.051849 -0.11816 0.15811 -0.29746 0.10336 -0.086949 -0.21092 0.041587 0.026308 0.058196 -0.1165 -0.10648 0.020896 0.08501 0.071356 -0.081837 0.14251 0.0098407 -0.23882 -0.20967 -0.25847 0.13381 0.028154 0.1986 -0.16473 0.05479 -0.098756 0.046418 -0.066485 -0.026495 0.03342 0.17899 0.03314 0.091089 \n",
            "en -0.068757 -0.35697 -0.29342 -0.094627 0.43517 -0.013682 0.011114 0.055615 -0.087329 0.12978 -0.074376 0.067942 0.20237 0.022203 -0.051031 0.12314 0.072765 -0.22897 0.16646 -0.084966 0.068295 -0.04783 0.11797 0.051383 -0.19304 -0.046978 -0.092263 0.066349 -0.17314 0.053773 -0.11937 0.026336 0.15324 -0.20365 0.0014295 -0.052225 -0.026814 0.15833 0.07017 0.095847 -0.17285 0.010883 0.12354 -0.13396 -0.052283 -0.027503 0.054485 -0.1647 0.29948 0.026969 0.18826 0.062579 0.07279 0.061239 0.061055 -0.11539 -0.12863 0.032541 0.10888 0.10692 0.13146 0.068959 0.13199 0.013904 0.012069 -0.010972 0.028878 -0.012772 0.011245 0.046867 -0.15217 -0.092395 -0.0887 0.12688 -0.06978 0.056482 -0.047862 0.16501 0.27966 -0.26591 -0.21623 -0.060548 -0.38319 -0.14658 -0.12514 -0.072977 0.13745 -0.079249 0.057424 0.098981 -0.17527 0.16346 0.011995 0.1198 0.11525 0.0050488 0.12991 0.12488 -0.012137 0.037292 -0.12015 0.25248 0.010958 -0.032322 -0.10779 0.061466 -0.033532 0.1955 -0.14306 0.059254 0.026576 0.11208 0.0042973 0.036137 -0.059411 0.076582 -0.27074 0.28835 -0.0079729 0.071278 0.037877 0.021052 -0.19483 -0.032827 -0.14709 -0.11128 0.0061628 -0.067146 -0.03106 -0.020031 -0.1113 0.1124 0.037424 -0.27515 0.068525 -0.091653 -0.17875 -0.092706 -0.041393 0.20776 0.017224 0.098478 0.053306 -0.019921 0.091692 -0.023147 -0.17799 -0.0943 -0.16926 -0.035759 0.19875 -0.14914 0.052972 -0.0048635 -0.10516 -0.084256 0.10061 -0.023569 -0.023324 0.0072619 0.038503 -0.10586 0.1278 -0.12241 -0.024696 -0.014071 -0.12414 0.13342 -0.03575 0.11323 0.064659 0.029628 0.15111 0.088225 0.22478 -0.36937 -0.10665 -0.1472 0.1741 0.11177 0.076097 -0.15853 0.080826 -0.077984 -0.10996 0.046357 -0.24808 -0.014166 -0.2103 -0.10641 -0.040944 -0.17095 -0.02888 0.022945 0.10056 -0.11606 0.047865 -0.29875 0.19779 0.078871 0.19898 0.090677 -0.15593 -0.10884 0.090468 -0.1389 -0.043738 0.025768 -0.077728 0.26161 -0.10919 0.0080624 0.072737 0.37907 -0.10518 0.021119 0.059352 0.09918 0.21482 -0.1947 0.18149 -0.04129 -0.1191 0.16121 0.065715 0.013441 -0.10234 0.0065899 0.1139 -0.079736 -0.04854 -0.052458 0.18536 0.095447 0.030906 0.1563 0.022803 0.17877 0.10853 -0.036586 -0.00063133 0.12533 -0.036867 -0.0035041 -0.073884 -0.065522 -0.37258 0.026007 -0.011914 0.085094 -0.10696 0.22082 0.049439 0.02624 -0.21721 -0.021654 0.019535 -0.14002 -0.065771 -0.036644 0.045191 -0.1199 -0.073064 0.15342 0.14735 0.079009 0.07846 -0.1406 0.10082 -0.098619 0.14948 -0.21718 0.0059333 0.031379 -0.13839 0.036691 -0.0097708 -0.21384 -0.19783 0.34342 -0.056492 -0.15188 -0.017733 -0.094947 -0.20862 -0.11989 -0.032559 0.19844 -0.0074601 0.13354 -0.23322 0.091892 -0.092023 -0.11707 -0.079512 0.15068 -0.03538 0.13903 0.12705 0.039991 \n",
            "y 0.023442 -0.088497 -0.029235 -0.089417 0.14933 0.01785 0.24729 0.036692 -0.12272 0.13642 -0.14477 0.17258 0.030684 -0.010674 -0.11353 0.1204 -0.02181 0.0019011 0.0092863 -0.023892 -0.038812 0.12652 0.060592 0.10523 0.041926 -0.14551 0.062859 -0.0661 -0.015429 0.20013 -0.084457 -0.016973 -0.037355 -0.099143 0.0091162 -0.08012 -0.091662 -0.016657 -0.1028 0.17398 -0.11465 0.093406 0.071148 -0.15152 -0.096375 0.069114 0.037463 -0.021307 -0.00085309 0.072409 0.11033 0.053209 -0.022796 -0.035695 -0.001883 -0.04891 -0.087402 0.12476 -0.048875 -0.045842 0.18206 0.023229 -0.018464 -0.095095 -0.077175 0.04364 0.098074 0.039214 -0.027157 -0.07585 0.026161 0.044686 -0.1244 0.070843 -0.078942 0.010978 0.012292 0.05039 0.1239 -0.059225 0.011614 -0.1406 0.00022614 0.0047793 -0.12093 0.0081183 0.079888 -0.24404 -0.016804 -0.054733 -0.1655 0.17767 -0.019322 0.058916 0.13053 -0.021297 -0.062198 0.015425 -0.010914 -0.09856 -0.11753 0.18514 0.085444 0.25258 -0.074872 0.028265 -0.063801 0.015337 -0.009768 0.01659 0.029629 0.15144 0.000195 0.13353 -0.13673 0.04354 0.0040654 0.069595 -0.067978 -0.096259 -0.034659 0.17558 0.054994 -0.057497 0.029118 0.047473 -0.040994 -0.10253 0.051201 0.042053 -0.0017651 0.25548 -0.065436 -0.14364 0.033991 0.093971 -0.23053 -0.020479 -0.072998 -0.0064869 -0.084871 0.15216 0.16647 0.0077729 0.072933 0.17275 0.095179 -0.027222 -0.0037222 -0.0046384 0.11661 -0.0051196 -0.052945 -0.08513 -0.15111 -0.069065 -0.017977 -0.080133 -0.026008 -0.052823 -0.018958 0.072747 -0.0046513 0.10876 -0.021448 -0.047137 0.0055715 0.1729 -0.11518 -0.069726 0.024805 -0.0173 -0.0049028 -0.099965 0.046767 -0.21582 -0.17057 -0.097863 0.13106 0.22625 -0.12044 -0.071834 -0.012569 0.09708 0.09035 0.18837 -0.064091 0.11535 0.0038907 -0.030121 0.028455 -0.087996 -0.18138 0.045254 0.11979 -0.038941 0.085323 -0.10879 0.040509 -0.10334 -0.023724 -0.045254 0.0062265 -0.017131 -5.126e-05 0.087498 -0.042371 0.12995 -0.10508 0.15582 -0.21643 0.10982 -0.031089 0.1506 -0.19333 0.06755 -0.003741 -0.076455 0.10062 -0.15313 -0.07235 -0.06462 -0.042583 0.007081 0.028273 0.03652 0.052481 -0.034572 -0.19099 -0.09198 0.027869 -0.13566 0.07899 0.065451 -0.19927 0.048687 -0.17581 0.13967 0.16728 0.025719 0.011307 -0.054724 0.09876 0.053774 -0.059775 0.075062 -0.090213 0.0025878 -0.042513 -0.036514 -0.10205 0.16386 0.047749 -0.093758 -0.097164 -0.059133 0.04216 0.019638 0.023031 -0.01261 0.017119 -0.10909 -0.027652 0.034062 0.18409 0.062573 -0.090312 0.054453 -0.0072908 -0.053816 -0.036657 -0.10774 -0.068853 -0.010237 -0.20078 -0.091738 0.036156 0.077155 -0.03432 0.019403 -0.017144 -0.14098 -0.082095 0.0031934 -0.08733 -0.18648 -0.24445 0.16713 0.093074 0.024423 -0.11458 0.15063 -0.028405 0.089409 0.023017 -0.00076226 0.08854 -0.029573 0.026931 0.025373 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "wordvectors_file_vec = 'unnotated.es.vec'\n",
        "cantidad = 100000\n",
        "wvFastTextSpanish2 = KeyedVectors.load_word2vec_format(wordvectors_file_vec, limit=cantidad)"
      ],
      "metadata": {
        "id": "i4lQc2rQebbP"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bf7a4f9-2b09-4aea-9edd-bc282ad41b99",
        "id": "ltsOQDiDfA7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('bici', 0.8596844673156738), ('motocicleta', 0.7343628406524658), ('moto', 0.7222307920455933), ('bicicletas', 0.7199786901473999), ('bicis', 0.6754755973815918)]\n"
          ]
        }
      ],
      "source": [
        "print(wvFastTextSpanish2.most_similar(positive=['bicicleta'], topn=5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e877662-6136-4439-efe4-a387f4454403",
        "id": "7ag5eExYfA7e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('actriz', 1.029092788696289),\n",
              " ('dramaturga', 0.9112269878387451),\n",
              " ('coreógrafa', 0.9106628894805908),\n",
              " ('locutora', 0.88897305727005),\n",
              " ('compositora', 0.8822876214981079),\n",
              " ('escritora', 0.8775303959846497),\n",
              " ('actrices', 0.8739449977874756),\n",
              " ('pintora', 0.8698908090591431),\n",
              " ('presentadora', 0.868553876876831),\n",
              " ('actoral', 0.8673722743988037)]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "wvFastTextSpanish2.most_similar_cosmul(positive=['actor','mujer'],negative=['hombre'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "154d899d-9cd9-4887-e29f-690593cc09f6",
        "id": "O_C3X7AKfA7e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('jugar', 0.9413612484931946),\n",
              " ('jugaba', 0.8551070690155029),\n",
              " ('jugando', 0.8297711610794067),\n",
              " ('juegan', 0.8239086866378784),\n",
              " ('jugaran', 0.8147255778312683),\n",
              " ('jugara', 0.8060213923454285),\n",
              " ('fútbol', 0.8019292950630188),\n",
              " ('jugado', 0.7989648580551147),\n",
              " ('jugaría', 0.7974690198898315),\n",
              " ('mediocampista', 0.7960951328277588)]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "wvFastTextSpanish2.most_similar_cosmul(positive=['cantar','juega'],negative=['canta'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a923bd76-6e6e-4e5c-ec93-1e6ff2376dc1",
        "id": "sxt9G_PHfA7e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('portugal', 0.9645054340362549),\n",
              " ('brasil', 0.8456214666366577),\n",
              " ('grecia', 0.8347657322883606),\n",
              " ('algarve', 0.8329377770423889),\n",
              " ('portuguesa', 0.8286518454551697),\n",
              " ('francia', 0.8207144141197205),\n",
              " ('polonia', 0.8112043738365173),\n",
              " ('italia', 0.8110285401344299),\n",
              " ('portuguesas', 0.8109676241874695),\n",
              " ('finlandia', 0.8105317950248718)]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "wvFastTextSpanish2.most_similar_cosmul(positive=['españa','lisboa'],negative=['madrid'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "a312dddd-6cc2-4765-b18a-8546f7c09aa8",
        "id": "xgG7EfZUfA7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py:895: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'septiembre'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "wvFastTextSpanish2.doesnt_match(['lunes', 'martes', 'septiembre', 'jueves', 'viernes'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfXNZ2Vd32Sq"
      },
      "source": [
        "Recuerda guardar tu notebook en GitHub con la opción *Save in GitHub* del menú *File*."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Practica9_1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}